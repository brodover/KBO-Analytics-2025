{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b301f58-5cea-416c-895b-f10986404cf1",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Request game data from text and record url, store as json files, and process stats into csv file\n",
    "\n",
    "from enum import IntEnum\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "import re # Needed for regular expression search\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "JSON_FOLDER = 'pitch_raw'\n",
    "CSV_FOLDER = 'pitch_processed'\n",
    "JSON_SUFFIX = 'raw'\n",
    "CSV_SUFFIX = 'processed'\n",
    "\n",
    "RECORD_URL = \"https://api-gw.sports.naver.com/schedule/games/{}/record\"\n",
    "RELAY_URL = \"https://api-gw.sports.naver.com/schedule/games/{}/relay?inning={}\"\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "}\n",
    "\n",
    "class Colors:\n",
    "    RED = '\\033[91m'       # High intensity Red\n",
    "    YELLOW = '\\033[93m'    # High intensity Yellow\n",
    "    ENDC = '\\033[0m'       # Reset to default color\n",
    "    \n",
    "class TextType(IntEnum):\n",
    "    INNING_START = 0 # including start of half inning\n",
    "    PITCH = 1\n",
    "    SUBSTITUTION = 2\n",
    "    TIMEOUT = 7 # Mound visit, VAR, pitcher leave mound, 우취\n",
    "    PA_START = 8\n",
    "    PA_RESULT_SELF = 13 # BB, HbP, Double play self out\n",
    "    PA_RESULT_RUNNER = 14 # BB, Steal, Double play runner out, runner base run\n",
    "    PA_RESULT_RBI_SELF = 23 # Hit, BB, \n",
    "    PA_RESULT_RBI_RUNNER = 24 # Hit, BB\n",
    "    FOUL_ERROR = 44 # Foul fly catch error\n",
    "    INNING_END = 99\n",
    "# ---------------------\n",
    "\n",
    "def log_error(message):\n",
    "    print(f\"{Colors.RED}CRITICAL ERROR: {message}{Colors.ENDC}\")\n",
    "\n",
    "def log_warning(message):\n",
    "    print(f\"{Colors.YELLOW}WARNING: {message}{Colors.ENDC}\")\n",
    "\n",
    "def extract_team_codes(game_id):\n",
    "    \"\"\"\n",
    "    Extracts the 2-character away and home team codes from the game_id string.\n",
    "    Example: '20250930LTHH02025' -> Away: 'LT', Home: 'HH'\n",
    "    \"\"\"\n",
    "    if len(game_id) < 12:\n",
    "        return 'N/A', 'N/A' # Handle unexpected format\n",
    "        \n",
    "    away_code = game_id[8:10]\n",
    "    home_code = game_id[10:12]\n",
    "    \n",
    "    return away_code, home_code\n",
    "        \n",
    "def _make_safe_request(url, max_retries=5):\n",
    "    \"\"\"\n",
    "    Performs a request with anti-blocking measures: random delay and exponential backoff.\n",
    "    Returns the Response object on success, or None on failure.\n",
    "    \"\"\"\n",
    "    # Start with a conservative wait time\n",
    "    wait_time = 2  \n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # 1. Randomized Delay (The critical anti-blocking measure)\n",
    "            delay = random.uniform(2.5, 9.5)\n",
    "            time.sleep(delay)\n",
    "\n",
    "            # 2. Make the request with standard headers\n",
    "            response = requests.get(url, headers=HEADERS, timeout=30)\n",
    "            \n",
    "            # 3. Handle success\n",
    "            if response.status_code == 200:\n",
    "                return response\n",
    "            \n",
    "            # 4. Handle \"Too Many Requests\" (429) using exponential backoff\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"[{url}] Received 429 on attempt {attempt + 1}. Waiting {wait_time}s and retrying.\")\n",
    "                time.sleep(wait_time)\n",
    "                wait_time *= 2  # Double the wait time (5, 10, 20, ...)\n",
    "            \n",
    "            # 5. Handle other HTTP errors (404, 500, etc.)\n",
    "            else:\n",
    "                response.raise_for_status() # Raises an exception for 4xx/5xx status codes\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            log_warning(f\"[{url}] Request failed on attempt {attempt + 1}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                # Wait before retrying on general network errors\n",
    "                time.sleep(wait_time)\n",
    "            \n",
    "        except Exception as e:\n",
    "             # Handle unexpected exceptions\n",
    "             log_error(f\"[{url}] Unexpected error: {e}\")\n",
    "             break\n",
    "\n",
    "    log_error(f\"[{url}] Failed to retrieve data after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "def _get_or_fetch_json(file_tag, month_folder, url_template, *url_args):\n",
    "    \"\"\"\n",
    "    Checks for a local JSON file based on the file_tag. \n",
    "    If not found or corrupted, fetches the data from the constructed URL.\n",
    "    \n",
    "    Args:\n",
    "        file_tag (str): A unique identifier for the file (e.g., 'gameId_record' or 'gameId_inning').\n",
    "        url_template (str): The format string for the API URL (e.g., RECORD_URL, RELAY_URL).\n",
    "        *url_args: Arguments to format the URL (e.g., game_id, inning_counter).\n",
    "\n",
    "    Returns:\n",
    "        dict: The loaded or fetched JSON data, or None if failed.\n",
    "    \"\"\"\n",
    "    \n",
    "    full_month_path = os.path.join(JSON_FOLDER, month_folder)\n",
    "    os.makedirs(full_month_path, exist_ok=True)\n",
    "    raw_file_path = os.path.join(full_month_path, f'{file_tag}_{JSON_SUFFIX}.json')\n",
    "    data = None\n",
    "    \n",
    "    # --- 1. CHECK FOR EXISTING FILE ---\n",
    "    if os.path.exists(raw_file_path):\n",
    "        try:\n",
    "            with open(raw_file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                print(f\"Loaded existing JSON\")\n",
    "                return data # Return immediately if successful\n",
    "        except json.JSONDecodeError as e:\n",
    "            log_error(f\"Error loading existing JSON file {raw_file_path}: {e}. Proceeding to refetch.\")\n",
    "        except Exception as e:\n",
    "            log_error(f\"An unexpected error occurred while reading {raw_file_path}: {e}. Proceeding to refetch.\")\n",
    "\n",
    "    # --- 2. FETCH NEW DATA ---\n",
    "    url = url_template.format(*url_args)\n",
    "\n",
    "    response = _make_safe_request(url)\n",
    "\n",
    "    if response and response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            \n",
    "            # Save the Raw JSON\n",
    "            with open(raw_file_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "            print(f\"  -> Successfully saved\")\n",
    "            return data\n",
    "                \n",
    "        except json.JSONDecodeError:\n",
    "            log_error(f\"Error decoding JSON from URL: {url}\")\n",
    "        \n",
    "    else:\n",
    "        # The request failed or returned a non-200 status\n",
    "        log_warning(f\"Failed to retrieve data from URL: {url}\")\n",
    "        \n",
    "    return None # Return None on any fetch or decode failure\n",
    "    \n",
    "def _extract_game_data(data):\n",
    "    \"\"\"Safely extracts max_inning and PA list from the JSON data.\"\"\"\n",
    "    text_relay_data = data.get('result', {}).get('textRelayData', {})\n",
    "\n",
    "    if text_relay_data is None:\n",
    "        return 0, None\n",
    "    \n",
    "    # max_inning needs to be extracted as an integer\n",
    "    try:\n",
    "        max_inning = int(text_relay_data.get('inn', 0))\n",
    "    except (TypeError, ValueError):\n",
    "        max_inning = 0\n",
    "        \n",
    "    pa_list = text_relay_data.get('textRelays', [])\n",
    "\n",
    "    # REVERSE THE LIST FOR CHRONOLOGICAL ORDER\n",
    "    pa_list.reverse() \n",
    "    \n",
    "    # The full data object is needed later to build the pitcher lookup (from inn 1 data)\n",
    "    return max_inning, pa_list\n",
    "    \n",
    "def get_record_json_file(game_id):\n",
    "    \"\"\"\n",
    "    Fetches/loads the game's record data and builds the batter/pitcher lookup tables.\n",
    "    \"\"\"\n",
    "    month_folder = game_id[4:6]\n",
    "    file_tag = f'{game_id}_record'\n",
    "    data = _get_or_fetch_json(file_tag, month_folder, RECORD_URL, game_id)\n",
    "\n",
    "    batter_record_lookup, pitcher_record_lookup = None, None\n",
    "    \n",
    "    if data:\n",
    "        record_data = data.get('result', {}).get('recordData', {})\n",
    "        if record_data and isinstance(record_data, dict):\n",
    "            batter_record_lookup, pitcher_record_lookup = _build_record_lookup(record_data)\n",
    "        else:\n",
    "            batter_record_lookup = {}\n",
    "            pitcher_record_lookup = {}\n",
    "    \n",
    "    return batter_record_lookup, pitcher_record_lookup\n",
    "\n",
    "def get_json_files(game_id):\n",
    "    \"\"\"\n",
    "    Fetches game data by inning, checks for existing files, and merges PA data.\n",
    "    Returns: master_json_list (list of all PAs) and pitcher_lookup.\n",
    "    \"\"\"\n",
    "    month_folder = game_id[4:6]\n",
    "    \n",
    "    inning_counter = 1\n",
    "    max_inning = 1\n",
    "    master_json_list = []\n",
    "    pitcher_lookup = None\n",
    "\n",
    "    while inning_counter <= max_inning:\n",
    "        print(f\"Processing Inning {inning_counter}...\")\n",
    "        file_tag = f'{game_id}_{inning_counter}'\n",
    "        data = _get_or_fetch_json(file_tag, month_folder, RELAY_URL, game_id, inning_counter)\n",
    "\n",
    "        if data is None:\n",
    "            # If fetch/load failed, stop the loop.\n",
    "            log_warning(f\"Failed to retrieve data for Inning {inning_counter}. Stopping.\")\n",
    "            break\n",
    "        \n",
    "        if data:\n",
    "            current_max_inning, pa_list = _extract_game_data(data)\n",
    "\n",
    "            if current_max_inning < 4:\n",
    "                print(\"Game canceled\")\n",
    "                break\n",
    "            \n",
    "            if inning_counter == 1:\n",
    "                # Update max_inning based on the first inning's data\n",
    "                max_inning = current_max_inning\n",
    "                pitcher_lookup = _build_pitcher_lookup(data)\n",
    "                print(f\"Max inning set to: {max_inning}\")\n",
    "\n",
    "            if max_inning == 0:\n",
    "                log_warning(f\"Unable to get a valid max_inning (is 0). Stopping.\")\n",
    "                break\n",
    "                \n",
    "            if not pa_list:\n",
    "                print(f\"Inning {inning_counter} contains no Plate Appearances. Stopping.\")\n",
    "                break\n",
    "            \n",
    "            # Collect the 'textRelays' list for later merging\n",
    "            master_json_list.extend(pa_list)\n",
    "            inning_counter += 1\n",
    "\n",
    "    print(f\"\\nFinished data collection. Total PA lists collected: {len(master_json_list)}\")\n",
    "    return master_json_list, pitcher_lookup\n",
    "\n",
    "def _build_record_lookup(data):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        dict: dictionary mapping player 'pcode' to their record data\n",
    "    \"\"\"\n",
    "    batter_record_lookup = {}\n",
    "    pitcher_record_lookup = {}\n",
    "    \n",
    "    batter_record = data.get('battersBoxscore', {})\n",
    "    pitcher_record = data.get('pitchersBoxscore', {})\n",
    "    away_batter_record = batter_record.get('away', [])\n",
    "    home_batter_record = batter_record.get('home', [])\n",
    "    away_pitcher_record = pitcher_record.get('away', [])\n",
    "    home_pitcher_record = pitcher_record.get('home', [])\n",
    "    \n",
    "    for p in away_batter_record:\n",
    "        pcode = p.get('playerCode')\n",
    "        if pcode:\n",
    "            batter_record_lookup[pcode] = p\n",
    "        \n",
    "    for p in home_batter_record:\n",
    "        pcode = p.get('playerCode')\n",
    "        if pcode:\n",
    "            batter_record_lookup[pcode] = p\n",
    "        \n",
    "    for p in away_pitcher_record:\n",
    "        pcode = p.get('playerCode')\n",
    "        if pcode:\n",
    "            pitcher_record_lookup[pcode] = p\n",
    "        \n",
    "    for p in home_pitcher_record:\n",
    "        pcode = p.get('playerCode')\n",
    "        if pcode:\n",
    "            pitcher_record_lookup[pcode] = p\n",
    "        \n",
    "    return batter_record_lookup, pitcher_record_lookup\n",
    "\n",
    "def _build_pitcher_lookup(full_inning_1_data):\n",
    "    \"\"\"\n",
    "    Extracts pitcher data from the full Inning 1 game object and creates a lookup dictionary.\n",
    "\n",
    "    Args:\n",
    "        full_inning_1_data (dict): The full JSON object from the first inning fetch.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping pitcher 'pcode' to their static data (name, stance, etc.).\n",
    "    \"\"\"\n",
    "    pitcher_lookup = {}\n",
    "    \n",
    "    try:\n",
    "        data = full_inning_1_data['result']['textRelayData']\n",
    "        home_pitcher_list = data['homeLineup']['pitcher']\n",
    "        away_pitcher_list = data['awayLineup']['pitcher']\n",
    "        all_pitchers_list = away_pitcher_list + home_pitcher_list\n",
    "        \n",
    "        for p in all_pitchers_list:\n",
    "            pcode = p.get('pcode')\n",
    "            if pcode:\n",
    "                # Pre-calculate stance for efficiency\n",
    "                hit_type = p.get('hitType', 'L')\n",
    "                p['stance_derived'] = 'R' if hit_type.startswith('우') else 'L'\n",
    "                pitcher_lookup[pcode] = p\n",
    "        \n",
    "        return pitcher_lookup\n",
    "        \n",
    "    except (KeyError, TypeError) as e:\n",
    "        log_error(f\"Error building pitcher lookup from lineup data: {e}\")\n",
    "        return {}\n",
    "\n",
    "# --- KINEMATICS AND ZONE CLASSIFICATION FUNCTIONS ---\n",
    "\n",
    "def calculate_plate_height(pitch_data):\n",
    "    \"\"\"Calculates the flight time and the corrected vertical position (z_plate).\"\"\"\n",
    "    try:\n",
    "        y0 = pitch_data['y0']\n",
    "        vy0 = pitch_data['vy0']\n",
    "        ay = pitch_data['ay']\n",
    "        z0 = pitch_data['z0']\n",
    "        vz0 = pitch_data['vz0']\n",
    "        az = pitch_data['az']\n",
    "    except KeyError as e:\n",
    "        # print(f\"Missing required key in pitch data: {e}\") # Suppress during bulk processing\n",
    "        return None\n",
    "\n",
    "    # Step 1: Calculate Flight Time (t)\n",
    "    a = 0.5 * ay\n",
    "    b = vy0\n",
    "    c = y0\n",
    "    discriminant = (b**2) - (4 * a * c)\n",
    "    \n",
    "    if discriminant < 0 or a == 0:\n",
    "        return {\"error\": \"Invalid kinematics data.\"}\n",
    "    \n",
    "    # Use the minus branch for the time to the plate (t > 0)\n",
    "    time_of_flight = (-b - math.sqrt(discriminant)) / (2 * a)\n",
    "    \n",
    "    # Step 2: Calculate Vertical Position (z_plate)\n",
    "    z_plate = z0 + (vz0 * time_of_flight) + (0.5 * az * (time_of_flight**2))\n",
    "\n",
    "    return {\n",
    "        \"time_of_flight\": time_of_flight,\n",
    "        \"z_plate\": z_plate\n",
    "    }\n",
    "\n",
    "def classify_5x5_zone(crossPlateX, calculated_z, topSz, bottomSz):\n",
    "    \"\"\"\n",
    "    Classifies a pitch into one of 25 zones (11 to 55) based on a 5x5 grid \n",
    "    (3x3 zone + 1 block shadow), and returns a flag if it is outside even the shadow.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Define Standard Baseball Constants\n",
    "    PLATE_WIDTH_FT = 1.4167  # 17 inches\n",
    "    HALF_PLATE = PLATE_WIDTH_FT / 2 # 0.70835\n",
    "    \n",
    "    # 2. Define Zone Block Sizes\n",
    "    X_BLOCK = PLATE_WIDTH_FT / 3\n",
    "    Z_BLOCK = (topSz - bottomSz) / 3\n",
    "    \n",
    "    # 3. Define X Boundaries (6 boundaries create 5 zones)\n",
    "    x_boundaries = [\n",
    "        -HALF_PLATE - X_BLOCK,   # X1: Far Left Shadow boundary\n",
    "        -HALF_PLATE,             # X2: Left edge of plate\n",
    "        -HALF_PLATE + X_BLOCK,   # X3: Left-center boundary\n",
    "        HALF_PLATE - X_BLOCK,    # X4: Right-center boundary\n",
    "        HALF_PLATE,              # X5: Right edge of plate\n",
    "        HALF_PLATE + X_BLOCK     # X6: Far Right Shadow boundary\n",
    "    ]\n",
    "    \n",
    "    # 4. Define Z Boundaries (6 boundaries create 5 zones)\n",
    "    z_boundaries = [\n",
    "        bottomSz - Z_BLOCK,      # Z1: Far Low Shadow boundary\n",
    "        bottomSz,                # Z2: Bottom of Zone\n",
    "        bottomSz + Z_BLOCK,      # Z3: Low-mid boundary\n",
    "        topSz - Z_BLOCK,         # Z4: Mid-high boundary\n",
    "        topSz,                   # Z5: Top of Zone\n",
    "        topSz + Z_BLOCK          # Z6: Far High Shadow boundary\n",
    "    ]\n",
    "    \n",
    "    # Determine the X-Index -1, (0 to 4), 5\n",
    "    x_index = -1\n",
    "    is_out_bound = False\n",
    "    for i, boundary in enumerate(x_boundaries):\n",
    "        if crossPlateX < boundary:\n",
    "            x_index = i\n",
    "            break\n",
    "    else:\n",
    "        x_index = 5\n",
    "\n",
    "    # Determine the Z-Index -1, (0 to 4), 5\n",
    "    z_index = -1\n",
    "    for i, boundary in enumerate(z_boundaries):\n",
    "        if calculated_z < boundary:\n",
    "            z_index = i\n",
    "            break\n",
    "    else:\n",
    "        z_index = 5\n",
    "\n",
    "\n",
    "    # --- Determine Outside Boundary Flag ---\n",
    "    # The flag is True if the pitch is outside the 5x5 grid (index 0 or 5)\n",
    "    is_outside_boundary = (x_index == 0) or (x_index == 5) or \\\n",
    "                              (z_index == 0) or (z_index == 5)\n",
    "    \n",
    "    # --- Calculate Final Zone ID (clamping index to 1-5) ---\n",
    "    # Clamp the index between 1 and 5 (index 0 maps to zone 1, index 5 maps to zone 5)\n",
    "    # This prevents the final Zone ID from being 00 or 66, but still flags the issue.\n",
    "    # Note: Zone ID 1x is the low row, 5x is the high row.\n",
    "    final_z_index = max(1, min(z_index, 5))\n",
    "    final_x_index = max(1, min(x_index, 5))\n",
    "\n",
    "    # Zone ID (e.g., Row 1 x 10 + Col 1 = 11, Row 5 x 10 + Col 5 = 55)\n",
    "    zone_id = final_z_index * 10 + final_x_index\n",
    "    \n",
    "    return {\n",
    "        \"zone_5x5_id\": str(zone_id),\n",
    "        \"is_outside_boundary\": is_outside_boundary,\n",
    "        \"raw_x_index\": x_index,\n",
    "        \"raw_z_index\": z_index\n",
    "    }\n",
    "\n",
    "def _count_pa_home_in(text):\n",
    "    \"\"\"\n",
    "    count home in during this pa, not from errors or steals, but from RBI\n",
    "    \"\"\"\n",
    "    home_in = '홈인'\n",
    "    home_run = '홈런'\n",
    "    error = '실책'\n",
    "    steal = '도루'\n",
    "\n",
    "    if ((home_in in text) | (home_run in text)) & (error not in text) & (steal not in text):\n",
    "        return 1\n",
    "\n",
    "    return 0\n",
    "\n",
    "def process_plate_appearance(game_id, pa_data, pitcher_lookup, away_code, home_code, batter_record_lookup, pitcher_record_lookup):\n",
    "    \"\"\"\n",
    "    Processes all events\n",
    "    within a single Plate Appearance (PA), correctly linking context to pitch data.\n",
    "    Returns a list of dictionaries, one for each processed pitch (type 1).\n",
    "    \"\"\"\n",
    "    processed_pitches_in_pa = []\n",
    "\n",
    "    # Create a MAP for Trajectory Data (ptsOptions)\n",
    "    # Key: pitchId (e.g., \"251007_151509\")\n",
    "    # Value: The full trajectory dict\n",
    "    trajectory_map = {\n",
    "        pitch.get('pitchId'): pitch\n",
    "        for pitch in pa_data.get('ptsOptions', [])\n",
    "    }\n",
    "    \n",
    "    # Contextual variables to be updated by Type 8 events\n",
    "    current_batter_name = 'N/A'\n",
    "    current_batter_lineup_pos = 'N/A'\n",
    "    current_batter_id = 'N/A' \n",
    "    is_batter_home = pa_data.get('homeOrAway') == \"1\"\n",
    "    inn = pa_data.get('inn', 0)\n",
    "\n",
    "    # Determine static PA team codes\n",
    "    batter_team_code = home_code if is_batter_home else away_code\n",
    "    pitcher_team_code = away_code if is_batter_home else home_code\n",
    "\n",
    "    # Check for pitcher change SUBSTITUTION\n",
    "    #pitcher_data = pitcher_lookup.get(pitcher_id, {})\n",
    "    #pitcher_name = pitcher_data.get('name', 'N/A')\n",
    "    #pitcher_stance = pitcher_data.get('stance_derived', 'L') # Using pre-derived stance\n",
    "\n",
    "    text_options = pa_data.get('textOptions', [])\n",
    "    pa_result_long = 'N/A'\n",
    "    pa_result_base1 = 'N/A'\n",
    "    pa_result_base2 = 'N/A'\n",
    "    pa_result_base3 = 'N/A'\n",
    "    pa_result_runs = 0\n",
    "    \n",
    "    # Iterate backward through Text Events (to get results)\n",
    "    is_first_iteration = True\n",
    "    for detail in reversed(text_options):\n",
    "        if is_first_iteration:\n",
    "            currentGameState = detail.get('currentGameState', {})\n",
    "            pa_result_base1 = currentGameState.get('base1')\n",
    "            pa_result_base2 = currentGameState.get('base2')\n",
    "            pa_result_base3 = currentGameState.get('base3')\n",
    "            is_first_iteration = False\n",
    "        \n",
    "        event_type = detail.get('type')\n",
    "        match event_type:\n",
    "            # even if inning ends not because of batter, need to record pa_result_base?\n",
    "            case TextType.PA_RESULT_RUNNER:\n",
    "                #pa_result = detail.get('text')\n",
    "                pa_result = ''\n",
    "            case TextType.PA_RESULT_RBI_RUNNER:\n",
    "                text = detail.get('text').split(\": \",1)[1]\n",
    "                pa_result_runs += _count_pa_home_in(text)\n",
    "            case TextType.PA_RESULT_SELF | TextType.PA_RESULT_RBI_SELF:\n",
    "                pa_result_long = detail.get('text').split(\": \",1)[1]\n",
    "                pa_result_runs += _count_pa_home_in(pa_result_long)\n",
    "            case TextType.PITCH:\n",
    "                break\n",
    "\n",
    "    before_strike = 'N/A'\n",
    "    before_ball = 'N/A'\n",
    "    before_out = 'N/A'\n",
    "    # Iterate forward through Text Events (textOptions)\n",
    "    for detail in text_options:\n",
    "        event_type = detail.get('type')\n",
    "        match event_type:\n",
    "            case TextType.PA_START:\n",
    "                batter_record = detail.get('batterRecord', {})\n",
    "                if not batter_record:\n",
    "                    break\n",
    "                \n",
    "                current_batter_name = batter_record.get('name', 'N/A')\n",
    "                current_batter_lineup_pos = batter_record.get('batOrder', 'N/A')\n",
    "                current_batter_id = batter_record.get('pcode', 'N/A')\n",
    "            case TextType.PITCH:\n",
    "                pitch_id = detail.get('ptsPitchId')\n",
    "                pitch = trajectory_map.get(pitch_id)\n",
    "    \n",
    "                # --- A. MERGE DATA & CALCULATIONS ---\n",
    "                \n",
    "                pitch_summary = {}\n",
    "                pitch_summary['game_id'] = game_id\n",
    "                \n",
    "                pitch_summary['plate_z_ft'] = None\n",
    "                zone_5x5_id = 0\n",
    "                is_outside_boundary = False\n",
    "                \n",
    "                # This pitch event exists in textOptions but has Trajectory Data (ptsOptions)\n",
    "                if pitch is None:\n",
    "                    pitch_summary['pitchId'] = pitch_id\n",
    "                    pitch_summary['inn'] = inn\n",
    "                else:\n",
    "                    pitch_summary.update(pitch)\n",
    "                    # Kinematic Calculations\n",
    "                    calculation_result = calculate_plate_height(pitch)\n",
    "\n",
    "                    if calculation_result:\n",
    "                        calculated_z = calculation_result.get('z_plate')\n",
    "                        zone_results = classify_5x5_zone(\n",
    "                            pitch.get('crossPlateX', 0.0), calculated_z, \n",
    "                            pitch.get('topSz', 3.3), pitch.get('bottomSz', 1.6)\n",
    "                        )\n",
    "                        pitch_summary['plate_z_ft'] = round(calculated_z, 4)\n",
    "                        zone_5x5_id = zone_results['zone_5x5_id']\n",
    "                        is_outside_boundary = zone_results['is_outside_boundary']\n",
    "    \n",
    "                # --- B. ADD CONTEXTUAL & PITCH DATA ---\n",
    "                \n",
    "                pitcher_id = currentGameState.get('pitcher')\n",
    "                pitcher_data = pitcher_lookup.get(pitcher_id, {})\n",
    "    \n",
    "                # 1. Game State\n",
    "                pitch_summary['is_batter_home'] = is_batter_home\n",
    "                pitch_summary['home_score'] = currentGameState.get('homeScore')\n",
    "                pitch_summary['away_score'] = currentGameState.get('awayScore')\n",
    "                pitch_summary['strike'] = before_strike\n",
    "                pitch_summary['ball'] = before_ball\n",
    "                pitch_summary['out'] = before_out\n",
    "                pitch_summary['base1'] = currentGameState.get('base1')\n",
    "                pitch_summary['base2'] = currentGameState.get('base2')\n",
    "                pitch_summary['base3'] = currentGameState.get('base3')\n",
    "    \n",
    "                # 3. Pitcher Info (From currentGameState)\n",
    "                pitch_summary['pitcher_id'] = pitcher_id\n",
    "                hit_type = pitcher_data.get('hitType', 'L') # Check for the '우' (Right) character in 'hitType' X투X타\n",
    "                pitch_summary['pitcher_stance'] = 'R' if hit_type.startswith('우') else 'L'\n",
    "                pitch_summary['pitcher_team_code'] = pitcher_team_code\n",
    "                pitch_summary['pitcher_name'] = pitcher_data.get('name', 'N/A')\n",
    "            \n",
    "                # 2. Batter Info (From Type 8 event)\n",
    "                pitch_summary['batter_id'] = current_batter_id\n",
    "                pitch_summary['batter_team_code'] = batter_team_code\n",
    "                pitch_summary['batter_lineup_pos'] = current_batter_lineup_pos\n",
    "                pitch_summary['batter_name'] = current_batter_name\n",
    "    \n",
    "                # 4. Pitch Details\n",
    "                pitch_summary['is_throwing_stretch'] = (pitch_summary['base1'] != '0') or \\\n",
    "                                                     (pitch_summary['base2'] != '0') or \\\n",
    "                                                     (pitch_summary['base3'] != '0')\n",
    "                \n",
    "                pitch_summary['pitch_type'] = detail.get('stuff', 'N/A')\n",
    "                pitch_summary['pitch_speed_kph'] = detail.get('speed', 'N/A')\n",
    "                pitch_summary['pitch_result'] = detail.get('pitchResult', 'N/A')\n",
    "                pitch_summary['is_outside_boundary'] = is_outside_boundary\n",
    "                pitch_summary['zone_5x5_id'] = zone_5x5_id\n",
    "\n",
    "                batter_record = batter_record_lookup.get(current_batter_id, {})\n",
    "                pa_result_short = batter_record.get(f'inn{inn}', 'N/A')\n",
    "                if pa_result_short == 'N/A':\n",
    "                    log_warning(f\"current_batter_id NOT FOUND: {inn}, {detail.get('text', 'N/A')}, {current_batter_name}, {current_batter_lineup_pos}, {current_batter_id}\")\n",
    "\n",
    "                pitch_summary['pa_result_long'] = pa_result_long\n",
    "                pitch_summary['pa_result_short'] = pa_result_short\n",
    "                pitch_summary['pa_result_base1'] = pa_result_base1\n",
    "                pitch_summary['pa_result_base2'] = pa_result_base2\n",
    "                pitch_summary['pa_result_base3'] = pa_result_base3\n",
    "                pitch_summary['pa_result_runs'] = pa_result_runs\n",
    "                \n",
    "                processed_pitches_in_pa.append(pitch_summary)\n",
    "            case TextType.PA_RESULT_SELF | TextType.PA_RESULT_RBI_SELF:\n",
    "                break\n",
    "            case TextType.INNING_START | TextType.SUBSTITUTION | TextType.TIMEOUT | \\\n",
    "                    TextType.PA_RESULT_RUNNER | TextType.PA_RESULT_RBI_RUNNER | \\\n",
    "                    TextType.INNING_END | TextType.FOUL_ERROR:\n",
    "                pass\n",
    "            case _:\n",
    "                log_warning(f\"Unknown Text Type: {event_type} {detail.get('text')}\")\n",
    "        \n",
    "        currentGameState = detail.get('currentGameState', {})\n",
    "        before_strike = currentGameState.get('strike')\n",
    "        before_ball = currentGameState.get('ball')\n",
    "        before_out = currentGameState.get('out')\n",
    "    \n",
    "    return processed_pitches_in_pa\n",
    "\n",
    "def main_processing_script(game_id):\n",
    "    away_code, home_code = extract_team_codes(game_id)\n",
    "    batter_record_lookup, pitcher_record_lookup = get_record_json_file(game_id)\n",
    "    master_pa_list, pitcher_lookup = get_json_files(game_id)\n",
    "\n",
    "    if not master_pa_list:\n",
    "        print(\"No plate appearances found. Exiting.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_processed_pitches = []\n",
    "\n",
    "    # 3. Process all Plate Appearances\n",
    "    for i, pa_data in enumerate(master_pa_list):\n",
    "        pitches_in_pa = process_plate_appearance(game_id, pa_data, pitcher_lookup, away_code, home_code, batter_record_lookup, pitcher_record_lookup)\n",
    "        all_processed_pitches.extend(pitches_in_pa)\n",
    "\n",
    "    print(f\"Total pitches processed: {len(all_processed_pitches)}\")\n",
    "\n",
    "    # 4. Convert to Pandas DataFrame and Clean\n",
    "    df = pd.DataFrame(all_processed_pitches)\n",
    "\n",
    "    df.rename(columns={\n",
    "        'pitchId': 'pitch_id',\n",
    "        'crossPlateX': 'plate_x_ft',\n",
    "        'ballcount': 'ball_count',\n",
    "        'crossPlateY': 'plate_y_ft',\n",
    "        'topSz': 'strikezone_top',\n",
    "        'bottomSz': 'strikezone_btm',\n",
    "        'stance': 'batter_stance',\n",
    "        'inn': 'inning'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # 5. Output the Results\n",
    "    if not df.empty:\n",
    "        output_filename = f'./{CSV_FOLDER}/{game_id}_{CSV_SUFFIX}.csv'\n",
    "        df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"\\n--- Final Processed DataFrame Saved to {output_filename} ---\")\n",
    "        print(f\"No. of Columns: {len(all_processed_pitches[0])}\")\n",
    "    else:\n",
    "        log_warning(f\"\\n--- DF is empty. Skipping output. ---\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_lotte_game_ids(start_date_str: str, end_date_str: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Scrapes Naver Sports KBO schedule pages month-by-month to find unique game IDs \n",
    "    for a specified team within a given date range.\n",
    "\n",
    "    Args:\n",
    "        team_code (str, optional): The KBO team code to filter for. Defaults to 'LT' (Lotte).\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A sorted list of unique KBO game IDs.\n",
    "    \"\"\"\n",
    "    all_game_ids: List[str] = list()\n",
    "    \n",
    "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "\n",
    "    current_date = start_date.replace(day=1)\n",
    "    \n",
    "    # 1. Iterate through the required months\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "        month_string = current_date.strftime('%m')\n",
    "\n",
    "        print(f\"Retrieving month record for {date_str}\")\n",
    "        schedule_url = f\"https://api-gw.sports.naver.com/schedule/calendar?upperCategoryId=kbaseball&categoryIds=kbo&date={date_str}\"\n",
    "        file_tag = f'{date_str}_month_matches'\n",
    "        data = _get_or_fetch_json(file_tag, month_string, schedule_url)\n",
    "\n",
    "        if data:\n",
    "            try:\n",
    "                dates_list = data.get('result').get('dates', [])\n",
    "                print(len(dates_list))\n",
    "                for date in dates_list:\n",
    "                    game_id_list = date.get('gameIds', [])\n",
    "                    all_game_ids.extend(game_id_list)\n",
    "                            \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                log_error(f\"Error fetching data for {url}: {e}\")\n",
    "                continue\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_error(f\"An unexpected error occurred while parsing {url}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            log_warning(\"Response is bad.\")\n",
    "        \n",
    "        # Move to the next month for iteration\n",
    "        if current_date.month == 12:\n",
    "            current_date = current_date.replace(year=current_date.year + 1, month=1, day=1)\n",
    "        else:\n",
    "            current_date = current_date.replace(month=current_date.month + 1, day=1)\n",
    "\n",
    "        # Stop if we've passed the end date\n",
    "        if current_date.strftime(\"%Y%m\") > end_date.strftime(\"%Y%m\") and current_date.day == 1:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nFound {len(all_game_ids)} unique game IDs.\")\n",
    "    print(all_game_ids)\n",
    "    return sorted(all_game_ids)\n",
    "\n",
    "# --- Execution Example ---\n",
    "if __name__ == '__main__':\n",
    "    # takes month and year, but ignores day\n",
    "    START_DATE = '2023-03-01'\n",
    "    END_DATE = '2023-11-01'\n",
    "\n",
    "    all_lotte_ids = get_lotte_game_ids(START_DATE, END_DATE)\n",
    "    total_count = len(all_lotte_ids)\n",
    "    current_count = 1\n",
    "    for game_id in all_lotte_ids:\n",
    "        print(f\"\\nProcessing {current_count} of {total_count}: Game ID {game_id}\")\n",
    "        main_processing_script(game_id)\n",
    "        current_count += 1\n",
    "\n",
    "    print(\"\\n--- All games processed successfully! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d72f4d-7d85-483d-be5e-bb3d1dfaf64b",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# validating collected data against naver's scoreboard\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import common as cmn\n",
    "\n",
    "KBO_PA_RESULT_MAP = {\n",
    "    # --- Walks & HBP (BB / HBP) ---\n",
    "    '4구': 'BB',    # Walk (Base on Balls)\n",
    "    '고4': 'BB',     # Intentional Walk (고의사구) - analytically still a BB\n",
    "    '사구': 'BB',   # Hit By Pitch (HBP) - analytically still a BB\n",
    "\n",
    "    # --- Strikeout (K) ---\n",
    "    '삼진': 'K',     # Strikeout\n",
    "    '스낫': 'NK',      # Strike not out (Dropped Third Strike) - counts as K, but not Out\n",
    "\n",
    "    # --- Special/Other ---\n",
    "    '야선': 'FC',    # Fielder's Choice (야수선택)\n",
    "}\n",
    "    \n",
    "# --- Helper Functions for Validation ---\n",
    "\n",
    "def standardize_pa_result(kbo_shorthand):\n",
    "    \"\"\"Converts KBO shorthand to a standard analytical result type.\"\"\"\n",
    "    if pd.isna(kbo_shorthand):\n",
    "        return None\n",
    "        \n",
    "    kbo_shorthand = str(kbo_shorthand).strip().upper()\n",
    "\n",
    "    # 1. Check for perfect matches\n",
    "    if kbo_shorthand in KBO_PA_RESULT_MAP:\n",
    "        return KBO_PA_RESULT_MAP[kbo_shorthand]\n",
    "\n",
    "    # 2. Check for partial matches or specific patterns\n",
    "    \n",
    "    # Pattern for Field-Specific Outs\n",
    "    # These often end in a common type: 땅(GO), 비(FO), 직(LO)\n",
    "    if kbo_shorthand.endswith('땅'):\n",
    "        return 'GO'\n",
    "    if kbo_shorthand.endswith('병'): # 병살\n",
    "        return 'GO'\n",
    "    if kbo_shorthand.endswith('비'): # 비행\n",
    "        return 'FO'\n",
    "    if kbo_shorthand.endswith('파'): # 파울플라이\n",
    "        return 'FO'\n",
    "    if kbo_shorthand.endswith('직'):\n",
    "        return 'LO'\n",
    "    if kbo_shorthand.endswith('희'): # 희생\n",
    "        return 'SAC'\n",
    "    if kbo_shorthand.endswith('희번'): # 희생 번트\n",
    "        return 'SAC'\n",
    "    if kbo_shorthand.endswith('실'): # 실책\n",
    "        return 'E'\n",
    "        \n",
    "    if kbo_shorthand.endswith('안'):\n",
    "        return '1B'\n",
    "    if kbo_shorthand.endswith('2'):\n",
    "        return '2B'\n",
    "    if kbo_shorthand.endswith('3'):\n",
    "        return '3B'\n",
    "    if kbo_shorthand.endswith('홈'):\n",
    "        return 'HR'\n",
    "    \n",
    "    # Fallback for unknown codes (helps identify missing codes in the map)\n",
    "    print(f\"Warning: Unknown KBO shorthand '{kbo_shorthand}' encountered.\")\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "# missing 이닝, 자책\n",
    "def calculate_pitcher_stats(df):\n",
    "    # 1. Group by Pitcher and PA using the standardized column\n",
    "    # Use the standardized column for filtering duplicates and defining the PA result\n",
    "    pa_results = df.dropna(subset=['standard_result']).drop_duplicates(\n",
    "        subset=['pitcher_id', 'batter_id', 'inning', 'batter_team_code'], \n",
    "        keep='last' \n",
    "    )\n",
    "    \n",
    "    # Prepare runs data (using 'pa_result_runs')\n",
    "    pa_results['Runs_Allowed'] = pd.to_numeric(pa_results['pa_result_runs'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # 2. Total Pitch Count (Remains the same)\n",
    "    pitch_counts = df.groupby('pitcher_id')['pitch_id'].count().reset_index(name='Count')\n",
    "\n",
    "    # 3. Plate Appearance Results (K, BB, H, AB) using the standardized codes\n",
    "    pitcher_pa_stats = pa_results.groupby('pitcher_id').agg(\n",
    "        # HITS (안타)\n",
    "        H=('standard_result', lambda x: (x.isin(['1B', '2B', '3B', 'HR'])).sum()),\n",
    "        # HOME RUNS ALLOWED (피홈런)\n",
    "        HR=('standard_result', lambda x: (x == 'HR').sum()),\n",
    "        # WALKS (볼넷)\n",
    "        BB=('standard_result', lambda x: (x == 'BB').sum()),\n",
    "        # STRIKEOUTS (삼진)\n",
    "        K=('standard_result', lambda x: (x.isin(['K', 'NK'])).sum()),\n",
    "        # HIT BY PITCH (사구)\n",
    "        HBP=('standard_result', lambda x: (x == 'HBP').sum()),\n",
    "        # SACRIFICE (희생타)\n",
    "        SF_SAC=('standard_result', lambda x: (x.isin(['SF', 'SAC'])).sum()),\n",
    "        # OUTS RECORDED\n",
    "        Out=('standard_result', \n",
    "                       lambda x: (~x.isin(['1B', '2B', '3B', 'HR', 'BB', 'HBP', 'E', 'NK'])).sum()),\n",
    "        # TOTAL PLATE APPEARANCES (타석)\n",
    "        PA=('standard_result', 'count'),\n",
    "        # TOTAL RUNS ALLOWED (실점)\n",
    "        R=('Runs_Allowed', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # 4. Calculate At Bats (타수)\n",
    "    pitcher_pa_stats['AB'] = pitcher_pa_stats['PA'] - pitcher_pa_stats['BB'] - \\\n",
    "                           pitcher_pa_stats['HBP'] - pitcher_pa_stats['SF_SAC']\n",
    "                           \n",
    "    # 5. Merge and Finalize (remaining steps are the same)\n",
    "    pitcher_report = pitch_counts.merge(pitcher_pa_stats, on='pitcher_id', how='left')\n",
    "\n",
    "    pitcher_name_map = df.drop_duplicates(subset=['pitcher_id']).set_index('pitcher_id')['pitcher_name'] \n",
    "    pitcher_report['Name'] = pitcher_report['pitcher_id'].map(pitcher_name_map)\n",
    "\n",
    "    pitcher_team_map = df.drop_duplicates(subset=['pitcher_id']).set_index('pitcher_id')['pitcher_team_code'] \n",
    "    pitcher_report['Team'] = pitcher_report['pitcher_id'].map(pitcher_team_map)\n",
    "    \n",
    "    # 6. Calculate IP (Innings Pitched) in the fractional X.Y format\n",
    "    pitcher_report['IP_Outs'] = pitcher_report['Out'] // 3\n",
    "    pitcher_report['IP_Remain'] = pitcher_report['Out'] % 3\n",
    "    # Format as string X.Y (e.g., 5.1, 5.2, 6.0)\n",
    "    pitcher_report['IP'] = pitcher_report['IP_Outs'].astype(str) + ',' + pitcher_report['IP_Remain'].astype(str)\n",
    "\n",
    "    # 7. Final Output\n",
    "    return pitcher_report[['Team', 'Name', 'IP', 'Count', 'PA', 'AB', 'H', 'HR', 'R', 'BB', 'K']].sort_values(by='Team', ascending=False)\n",
    "\n",
    "def calculate_batter_stats(df):\n",
    "    \"\"\"\n",
    "    Calculates key batter metrics (PA, R, H, RBI, HR, BB, K) \n",
    "    using standardized PA results and runs scored data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Ensure standardization has run (if not already done)\n",
    "    if 'standard_result' not in df.columns:\n",
    "        df['standard_result'] = df['pa_result_short'].apply(standardize_pa_result)\n",
    "\n",
    "    # 2. Filter for final PA results (one row per Plate Appearance)\n",
    "    pa_results = df.dropna(subset=['standard_result']).drop_duplicates(\n",
    "        subset=['pitcher_id', 'batter_id', 'inning', 'batter_team_code'], \n",
    "        keep='last'\n",
    "    )\n",
    "    \n",
    "    # This column must contain the total number of runs that scored *due to this PA*\n",
    "    pa_results['runs_on_play'] = pd.to_numeric(pa_results['pa_result_runs'], errors='coerce').fillna(0)\n",
    "\n",
    "\n",
    "    # 3. Aggregate Batter Statistics\n",
    "    batter_report = pa_results.groupby('batter_id').agg(\n",
    "        # TOTAL PLATE APPEARANCES (타석)\n",
    "        PA=('standard_result', 'count'),\n",
    "        # HITS (안타)\n",
    "        H=('standard_result', lambda x: (x.isin(['1B', '2B', '3B', 'HR'])).sum()),\n",
    "        # HOME RUNS (홈런) - subset of Hits\n",
    "        HR=('standard_result', lambda x: (x == 'HR').sum()),\n",
    "        # WALKS (볼넷) + HBP - Assuming 'BB' standard result is for both 4구, 고4, and 사구\n",
    "        BB=('standard_result', lambda x: (x == 'BB').sum()),\n",
    "        # STRIKEOUTS (삼진)\n",
    "        K=('standard_result', lambda x: (x == 'K').sum()),\n",
    "        # SACRIFICE (희생타) - Needed for AB calculation\n",
    "        SF_SAC=('standard_result', lambda x: (x.isin(['SF', 'SAC'])).sum()),\n",
    "        # RUNS BATTED IN (타점) - Total runs scored *on this PA* (requires pa_result_runs)\n",
    "        # We assume runs_on_play includes the batter if they hit a HR, but RBI calculation\n",
    "        # is typically runs_on_play minus the run scored by the batter if they score on non-HR.\n",
    "        # For simple validation, we use total runs scored on the play.\n",
    "        # Note: True RBI logic is complex (e.g., doesn't count if batter gets K/GO/FO on error).\n",
    "        # We will use a simple proxy for now:\n",
    "        RBI_PROXY=('runs_on_play', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # 4. Calculate At Bats (타수)\n",
    "    # AB = PA - BB - HBP - SF - SAC. Since 'BB' includes HBP in your map, we simplify.\n",
    "    # If HBP needs to be tracked separately for official AB calculation, you must adjust KBO_PA_RESULT_MAP\n",
    "    # to differentiate between 'BB' (walk) and 'HBP'. For now, we assume your map handles it correctly.\n",
    "    batter_report['AB'] = batter_report['PA'] - batter_report['BB'] - batter_report['SF_SAC']\n",
    "\n",
    "    # 5. Get Runs Scored (R)\n",
    "    # This requires identifying when a runner (including the batter) crossed home plate.\n",
    "    # The simplest proxy: runs scored is R = sum of all runs scored on PAs by the *player's team* # when the *player* was the batter, where the batter also scores if they hit a HR.\n",
    "    # The true R stat is complex, so we will skip it for this first pass of validation.\n",
    "    # If you need R, you'll need a separate column that tracks whether the batter themselves scored (usually if they hit a HR and the bases weren't loaded).\n",
    "    # For now, we'll keep the focus on *offensive production*: H, HR, RBI.\n",
    "    \n",
    "    # 6. Map Player Info (using the corrected mapping logic)\n",
    "    batter_name_map = df.drop_duplicates(subset=['batter_id']).set_index('batter_id')['batter_name']\n",
    "    batter_report['Name'] = batter_report['batter_id'].map(batter_name_map)\n",
    "    \n",
    "    batter_team_map = df.drop_duplicates(subset=['batter_id']).set_index('batter_id')['batter_team_code']\n",
    "    batter_report['Team'] = batter_report['batter_id'].map(batter_team_map)\n",
    "\n",
    "    # 7. Final Output Cleanup and Rename\n",
    "    batter_report = batter_report.rename(columns={'RBI_PROXY': 'RBI', 'K': 'SO'})\n",
    "\n",
    "    return batter_report[['Team', 'Name', 'PA', 'AB', 'H', 'HR', 'RBI', 'BB', 'SO']].sort_values(by='Team', ascending=False)\n",
    "\n",
    "game_id = '20250629KTLT02025'\n",
    "df = pd.read_csv(f\"pitch_processed/{game_id}_processed.csv\")\n",
    "df['standard_result'] = df['pa_result_short'].apply(standardize_pa_result)\n",
    "\n",
    "pitcher_validation_df = calculate_pitcher_stats(df)\n",
    "batter_validation_df = calculate_batter_stats(df)\n",
    "\n",
    "print (f\"df length: {len(df)}\")\n",
    "print(f\"pitch total: {pitcher_validation_df['Count'].sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Pitcher Validation Report\")\n",
    "print(\"=\"*50)\n",
    "print(pitcher_validation_df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Batter Validation Report\")\n",
    "print(\"=\"*50)\n",
    "print(batter_validation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914b0ee-662f-4969-902d-7c6b9d9c6570",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot pitch\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm # For color maps\n",
    "\n",
    "def plot_pitch_trajectories(df, game_id):\n",
    "    \"\"\"\n",
    "    Generates a scatter plot of pitch locations (X vs Z) for all pitchers.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The pitch-by-pitch DataFrame.\n",
    "    \"\"\"\n",
    "    plt.rcParams['font.family'] = ['Noto Serif KR']  # Set the font family\n",
    "\n",
    "    x_col = 'plate_x_ft'\n",
    "    z_col = 'plate_z_ft'\n",
    "    pitch_type = 'pitch_type'\n",
    "    \n",
    "    # 1. Get all unique pitchers, their teams, and sort by team code\n",
    "    pitcher_info = df[['pitcher_id', 'pitcher_team_code', 'pitcher_name']].drop_duplicates(subset=['pitcher_id'])\n",
    "    pitcher_info = pitcher_info.sort_values(by=['pitcher_team_code', 'pitcher_id'])\n",
    "    \n",
    "    sample_pitchers = pitcher_info['pitcher_id'].tolist()\n",
    "    num_pitchers = len(sample_pitchers)\n",
    "    \n",
    "    if num_pitchers == 0: \n",
    "        print(\"Error: No unique pitchers found in the data.\")\n",
    "        return\n",
    "        \n",
    "    # 2. Set up the plotting grid (dynamic sizing)\n",
    "    # Use max 3 columns for readability, adjusting rows accordingly\n",
    "    cols = min(num_pitchers, 3) \n",
    "    rows = int(np.ceil(num_pitchers / cols))\n",
    "    \n",
    "    # Increase figsize dynamically\n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(5 * cols, 5 * rows), sharex=True, sharey=True)\n",
    "    axes = axes.flatten() if num_pitchers > 1 else [axes]\n",
    "    \n",
    "    # Handle cases where the number of subplots exceeds the number of pitchers\n",
    "    for i in range(num_pitchers, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "        \n",
    "    # Get average strike zone dimensions\n",
    "    sz_top_avg = df['strikezone_top'].mean()\n",
    "    sz_bot_avg = df['strikezone_btm'].mean()\n",
    "    sz_width = 1.416 # Standard 17 inches in feet (17/12)\n",
    "    \n",
    "    # Prepare colors for pitch types\n",
    "    unique_pitch_types = df[pitch_type].unique()\n",
    "    unique_pitch_types = [pt for pt in unique_pitch_types if pd.notna(pt)]\n",
    "    \n",
    "    # Use a colormap to get distinct colors\n",
    "    colors = cm.get_cmap('tab10', len(unique_pitch_types))\n",
    "    pitch_type_color_map = {pt: colors(i) for i, pt in enumerate(unique_pitch_types)}\n",
    "    \n",
    "    # 3. Iterate and plot each pitcher\n",
    "    for i, pitcher_id in enumerate(sample_pitchers):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Filter data for the current pitcher\n",
    "        pitcher_data = df[df['pitcher_id'] == pitcher_id].copy()\n",
    "        \n",
    "        info = pitcher_info[pitcher_info['pitcher_id'] == pitcher_id].iloc[0]\n",
    "        pitcher_name = info['pitcher_name']\n",
    "        pitcher_team = info['pitcher_team_code']\n",
    "        \n",
    "        # Scatter Plot - now colored by pitch type\n",
    "        for pt in unique_pitch_types:\n",
    "            subset = pitcher_data[pitcher_data[pitch_type] == pt]\n",
    "            ax.scatter(subset[x_col], subset[z_col], alpha=0.4, s=15, \n",
    "                       color=pitch_type_color_map.get(pt, 'gray'), label=pt) # Use 'gray' for unknown\n",
    "            \n",
    "        # --- Draw Strike Zone (The primary validation step) ---\n",
    "        # Draw the standard rectangle\n",
    "        rect = plt.Rectangle(\n",
    "            (-sz_width / 2, sz_bot_avg), # Bottom-left corner\n",
    "            sz_width,                    # Width\n",
    "            sz_top_avg - sz_bot_avg,     # Height\n",
    "            edgecolor='black', \n",
    "            facecolor='none', \n",
    "            lw=2, \n",
    "            zorder=5,\n",
    "            label='Strike Zone (Avg)'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # --- Draw 5x5 Grid Lines (Validation for zone_5x5_id) ---\n",
    "        #x_third = sz_width / 3\n",
    "        z_third = (sz_top_avg - sz_bot_avg) / 3\n",
    "\n",
    "        # Vertical lines (separating 9 zones in X)\n",
    "        ax.axvline(-sz_width / 6, color='gray', linestyle=':', lw=1, zorder=4)\n",
    "        ax.axvline(sz_width / 6, color='gray', linestyle=':', lw=1, zorder=4)\n",
    "        \n",
    "        # Horizontal lines (separating 9 zones in Z)\n",
    "        ax.axhline(sz_bot_avg + z_third, color='gray', linestyle=':', lw=1, zorder=4)\n",
    "        ax.axhline(sz_top_avg - z_third, color='gray', linestyle=':', lw=1, zorder=4)\n",
    "        \n",
    "        # --- Set Plot Aesthetics ---\n",
    "        ax.set_title(f\"{pitcher_name} ({pitcher_team})\", fontsize=10)\n",
    "        ax.set_xlabel(f'{x_col} (ft)')\n",
    "        ax.set_ylabel(f'{z_col} (ft)')\n",
    "        ax.legend(loc='lower left', fontsize=8)\n",
    "        \n",
    "        # Set uniform limits for a clear view\n",
    "        ax.set_xlim(-2.5, 2.5) \n",
    "        ax.set_ylim(0.0, 5.0) \n",
    "\n",
    "    fig.suptitle('Pitch Trajectories (X vs Z at Plate)', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97]) # Adjust layout for suptitle\n",
    "    plt.savefig(f'{game_id}_pitcher_trajectories.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Plot 'pitcher_trajectories.png' generated successfully.\")\n",
    "\n",
    "game_id = '20250930LTHH02025'\n",
    "#game_id = '20250926SSLT02025'\n",
    "df = pd.read_csv(f\"pitch_processed/{game_id}_processed.csv\")\n",
    "plot_pitch_trajectories(df, game_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5fa4fe-f0c3-44d6-97b9-a55ce5ae96d6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Check batters' O_Swing_% (non-strike swings) and Z_Swing_% (strike swings)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import common as cmn\n",
    "\n",
    "KBO_P_RESULT_KNOWN = set(cmn.PITCH_RESULT_MAP.keys())\n",
    "\n",
    "def _is_pitch_a_strike(row, horizontal_buffer=0.0):\n",
    "    \"\"\"\n",
    "    Determines if a pitch is a physical strike based on the ABS-defined zone \n",
    "    (personalized to the batter's height).\n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): A row from the pitch-by-pitch DataFrame.\n",
    "        horizontal_buffer (float): Optional buffer (in ft) for the plate width.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the pitch is physically within the strike zone, False otherwise.\n",
    "    \"\"\"\n",
    "    plate_x = row['plate_x_ft']\n",
    "    plate_z = row['plate_z_ft']\n",
    "    sz_top = row['strikezone_top']\n",
    "    sz_btm = row['strikezone_btm']\n",
    "    \n",
    "    # KBO standard plate width is 17 inches (1.416 ft). Half is 0.708 ft.\n",
    "    half_plate = 0.708\n",
    "    \n",
    "    # 1. Vertical check (The core of the ABS zone)\n",
    "    is_vertical_strike = (plate_z <= sz_top) and (plate_z >= sz_btm)\n",
    "    \n",
    "    # 2. Horizontal check (Standard 17-inch width + optional buffer)\n",
    "    is_horizontal_strike = (plate_x >= (-half_plate - horizontal_buffer)) and \\\n",
    "                           (plate_x <= (half_plate + horizontal_buffer))\n",
    "                           \n",
    "    return is_vertical_strike and is_horizontal_strike\n",
    "\n",
    "\n",
    "def _infer_swing_decision(pitch_result: str) -> bool:\n",
    "    \"\"\"\n",
    "    Infers if a batter swung at a pitch based on the pitch result code \n",
    "    using the defined SWING_CODES set.\n",
    "    \"\"\"\n",
    "    if pitch_result not in KBO_P_RESULT_KNOWN:\n",
    "        print(f\"UNKNOWN P_RESULT: {pitch_result}\")\n",
    "    \n",
    "    return pitch_result in cmn.SWING_CODES\n",
    "\n",
    "\n",
    "def calculate_batter_discipline(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates O-Swing % (Chase Rate) and Z-Swing % (In-Zone Swing Rate) per batter \n",
    "    based on the objective ABS strike zone for a single game.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Pitch-by-pitch data for one game.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Batter plate discipline statistics.\n",
    "    \"\"\"\n",
    "    # Drop rows missing crucial location or zone data\n",
    "    df_clean = df.dropna(subset=['plate_x_ft', 'plate_z_ft', 'strikezone_top', 'strikezone_btm', 'pitch_result']).copy()\n",
    "    \n",
    "    if df_clean.empty:\n",
    "        print(\"Warning: Insufficient data after cleaning for discipline calculation.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # 1. Define Zone for Every Pitch\n",
    "    df_clean['is_ABS_Strike'] = df_clean.apply(_is_pitch_a_strike, axis=1)\n",
    "    \n",
    "    # 2. Infer Swing for Every Pitch\n",
    "    df_clean['is_swing'] = df_clean['pitch_result'].apply(_infer_swing_decision)\n",
    "    \n",
    "    # 3. Categorize Pitches\n",
    "    df_clean['is_Z_Pitch'] = df_clean['is_ABS_Strike']   # Inside Zone (Z)\n",
    "    df_clean['is_O_Pitch'] = ~df_clean['is_ABS_Strike']  # Outside Zone (O)\n",
    "    \n",
    "    # 4. Group and Aggregate\n",
    "    discipline_stats = df_clean.groupby(['batter_id', 'batter_team_code', 'batter_name']).agg(\n",
    "        # Swings\n",
    "        O_Swings=('is_swing', lambda x: x[df_clean.loc[x.index, 'is_O_Pitch']].sum()),\n",
    "        Z_Swings=('is_swing', lambda x: x[df_clean.loc[x.index, 'is_Z_Pitch']].sum()),\n",
    "        \n",
    "        # Total Pitches\n",
    "        Total_O_Pitches=('is_O_Pitch', 'sum'),\n",
    "        Total_Z_Pitches=('is_Z_Pitch', 'sum'),\n",
    "        \n",
    "        Total_Pitches=('pitch_id', 'count')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # 5. Calculate Percentages\n",
    "    \n",
    "    # O-Swing % (Chase Rate) = O_Swings / Total_O_Pitches\n",
    "    discipline_stats['O_Swing_%'] = (\n",
    "        (discipline_stats['O_Swings'] / discipline_stats['Total_O_Pitches']) * 100\n",
    "    ).round(2).fillna(0)\n",
    "    \n",
    "    # Z-Swing % = Z_Swings / Total_Z_Pitches\n",
    "    discipline_stats['Z_Swing_%'] = (\n",
    "        (discipline_stats['Z_Swings'] / discipline_stats['Total_Z_Pitches']) * 100\n",
    "    ).round(2).fillna(0)\n",
    "    \n",
    "    # 6. Final Output\n",
    "    return discipline_stats[['batter_id', 'batter_team_code', 'batter_name', 'Total_Pitches', \n",
    "                             'O_Swings', 'Total_O_Pitches', 'O_Swing_%', \n",
    "                             'Z_Swings', 'Total_Z_Pitches', 'Z_Swing_%']].sort_values(by='batter_team_code')\n",
    "\n",
    "game_id = '20250322LTLG02025'\n",
    "df = pd.read_csv(f\"pitch_processed/{game_id}_processed.csv\")\n",
    "calculate_batter_discipline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd2b0c-cbc6-4773-b60f-5ff5ba2300e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# generate _discipline.json and strikezone_summary.json for pitch-chart\n",
    "\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# --- Configuration and Helper Functions ---\n",
    "WIDTH = 860\n",
    "HEIGHT = 760\n",
    "RESULT_DOMAIN = ['Ball In Play', 'Foul', 'Swing', 'Called Strike', 'Ball']\n",
    "RESULT_RANGE = ['steelblue', 'mediumpurple', 'palevioletred', 'orange', 'mediumseagreen']\n",
    "PITCH_DOMAIN = list(cmn.PITCH_TYPE_MAP.keys())\n",
    "PITCH_RANGE = ['arrow', 'circle', 'diamond', 'triangle-left', 'triangle-down', 'triangle-up', 'cross', 'square', 'wedge']\n",
    "LOTTE_CODE = 'LT'\n",
    "STRIKEZONE_SUMMARY_FILE = \"strikezone_summary.json\"\n",
    "\n",
    "def map_pitch_to_plot_category(pitch_result: str) -> str:\n",
    "    \"\"\"Maps pitch result codes to descriptive categories for plotting.\"\"\"\n",
    "    if pd.isna(pitch_result): return 'Unknown'\n",
    "    pitch_result = str(pitch_result).upper()\n",
    "    if pitch_result in ['H']: return 'Ball In Play'\n",
    "    elif pitch_result in ['F', 'W']: return 'Foul'\n",
    "    elif pitch_result in ['S', 'V']: return 'Swing'\n",
    "    elif pitch_result in ['T']: return 'Called Strike'\n",
    "    elif pitch_result in ['B']: return 'Ball'\n",
    "    else: return 'Other'\n",
    "\n",
    "# --- DYNAMIC STRIKE ZONE LOGIC (generates strikezone_summary.json) ---\n",
    "def calculate_strikezone_averages(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_clean = df.dropna(subset=['strikezone_top', 'strikezone_btm']).copy()\n",
    "    agg_cols = {'strikezone_top': 'mean', 'strikezone_btm': 'mean'}\n",
    "    all_summaries = []\n",
    "\n",
    "    # 1. Overall Average\n",
    "    overall_stats = df_clean.agg(agg_cols)\n",
    "    all_summaries.append(pd.DataFrame([{ 'entity_type': 'ALL', 'entity_name': 'ALL', 'avg_strikezone_top': overall_stats['strikezone_top'], 'avg_strikezone_btm': overall_stats['strikezone_btm'] }]))\n",
    "\n",
    "    # 2. Average by Batter Team\n",
    "    team_avg = df_clean.groupby('batter_team_code').agg(agg_cols).reset_index()\n",
    "    team_avg.rename(columns={'batter_team_code': 'entity_name', 'strikezone_top': 'avg_strikezone_top', 'strikezone_btm': 'avg_strikezone_btm'}, inplace=True)\n",
    "    team_avg['entity_type'] = 'TEAM'\n",
    "    all_summaries.append(team_avg)\n",
    "\n",
    "    # 3. Average by Batter Name\n",
    "    batter_avg = df_clean.groupby('batter_name').agg(agg_cols).reset_index()\n",
    "    batter_avg.rename(columns={'batter_name': 'entity_name', 'strikezone_top': 'avg_strikezone_top', 'strikezone_btm': 'avg_strikezone_btm'}, inplace=True)\n",
    "    batter_avg['entity_type'] = 'BATTER'\n",
    "    all_summaries.append(batter_avg)\n",
    "\n",
    "    final_df = pd.concat(all_summaries, ignore_index=True)\n",
    "    numeric_cols = ['avg_strikezone_top', 'avg_strikezone_btm']\n",
    "    final_df[numeric_cols] = final_df[numeric_cols].round(4)\n",
    "    return final_df[['entity_type', 'entity_name', 'avg_strikezone_top', 'avg_strikezone_btm']]\n",
    "\n",
    "def generate_lotte_offense_chart(df: pd.DataFrame, opponent_code: str, output_filename: str) -> None:\n",
    "    df_plot = df[(df['batter_team_code'] == LOTTE_CODE) & (df['pitcher_team_code'] == opponent_code)].copy()\n",
    "    if df_plot.empty: return\n",
    "\n",
    "    df_plot.dropna(subset=['plate_x_ft', 'plate_z_ft', 'pitch_type', 'pitch_result', 'batter_name', 'pitcher_name', 'strikezone_top', 'strikezone_btm'], inplace=True)\n",
    "    df_plot['plot_result'] = df_plot['pitch_result'].apply(map_pitch_to_plot_category)\n",
    "    \n",
    "    # These are placeholder values for the initial JSON load\n",
    "    sz_top_avg = df_plot['strikezone_top'].mean()\n",
    "    sz_btm_avg = df_plot['strikezone_btm'].mean()\n",
    "    \n",
    "    create_discipline_chart(\n",
    "            df_plot, \n",
    "            sz_top_avg, \n",
    "            sz_btm_avg, \n",
    "            lotte_role='batter', \n",
    "            opponent_code=opponent_code,\n",
    "            output_filename=output_filename\n",
    "        )\n",
    "\n",
    "def generate_lotte_defense_chart(df: pd.DataFrame, opponent_code: str, output_filename: str) -> None:\n",
    "    df_plot = df[(df['pitcher_team_code'] == LOTTE_CODE) & (df['batter_team_code'] == opponent_code)].copy()\n",
    "    if df_plot.empty: return\n",
    "\n",
    "    df_plot.dropna(subset=['plate_x_ft', 'plate_z_ft', 'pitch_type', 'pitch_result', 'batter_name', 'pitcher_name', 'strikezone_top', 'strikezone_btm'], inplace=True)\n",
    "    df_plot['plot_result'] = df_plot['pitch_result'].apply(map_pitch_to_plot_category)\n",
    "    \n",
    "    # These are placeholder values for the initial JSON load\n",
    "    sz_top_avg = df_plot['strikezone_top'].mean()\n",
    "    sz_btm_avg = df_plot['strikezone_btm'].mean()\n",
    "\n",
    "    create_discipline_chart(\n",
    "        df_plot, \n",
    "        sz_top_avg, \n",
    "        sz_btm_avg, \n",
    "        lotte_role='pitcher', \n",
    "        opponent_code=opponent_code,\n",
    "        output_filename=output_filename\n",
    "    )\n",
    "\n",
    "def create_discipline_chart(\n",
    "    df_plot: pd.DataFrame, \n",
    "    sz_top_avg: float, \n",
    "    sz_btm_avg: float, \n",
    "    lotte_role: str, # 'batter' or 'pitcher'\n",
    "    opponent_code: str,\n",
    "    output_filename: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates the layered Altair chart, including the scatter plot,\n",
    "    strike zones, and 3x3 grid.\n",
    "    \"\"\"\n",
    "    # Calculate dimensions for zones and grid\n",
    "    sz_height = sz_top_avg - sz_btm_avg \n",
    "    sz_third_y = sz_height / 3.0 # Height of each third\n",
    "    half_plate = 0.708 # Half plate width in feet\n",
    "    sz_third_x = half_plate / 3.0 # Width of each third (approx 0.236)\n",
    "    \n",
    "    # Determine the primary and opponent selection fields\n",
    "    if lotte_role == 'batter':\n",
    "        opponent_role = 'pitcher'\n",
    "        lotte_field = 'batter_name'\n",
    "        opponent_field = 'pitcher_name'\n",
    "        lotte_players = df_plot[lotte_field].unique().tolist()\n",
    "        opponent_players = df_plot[opponent_field].unique().tolist()\n",
    "        title_base = f'{LOTTE_CODE} Batters Plate Discipline vs. {opponent_code} Pitchers'\n",
    "    else: # pitcher\n",
    "        opponent_role = 'batter'\n",
    "        lotte_field = 'pitcher_name'\n",
    "        opponent_field = 'batter_name'\n",
    "        lotte_players = df_plot[lotte_field].unique().tolist()\n",
    "        opponent_players = df_plot[opponent_field].unique().tolist()\n",
    "        title_base = f'{LOTTE_CODE} Pitchers Plate Discipline vs. {opponent_code} Batters'\n",
    "\n",
    "    # --- A. GRID AND ZONE LAYERS ---\n",
    "\n",
    "    # Vertical Grid Lines (Two Lines)\n",
    "    vertical_grid_data = pd.DataFrame({\n",
    "        'id': ['vertical_grid_1', 'vertical_grid_2'],\n",
    "        'x_center': [-sz_third_x, sz_third_x], \n",
    "        'y': [sz_btm_avg] * 2,\n",
    "        'y2': [sz_top_avg] * 2\n",
    "    })\n",
    "    vertical_grid = alt.Chart(vertical_grid_data).mark_rule(\n",
    "        color='red', strokeWidth=1, opacity=0.8, strokeDash=[5, 5]\n",
    "    ).encode(x='x_center', y='y', y2='y2')\n",
    "    \n",
    "    # Horizontal Grid Lines (Two Lines)\n",
    "    horizontal_grid_data = pd.DataFrame({\n",
    "        'id': ['horizontal_grid_1', 'horizontal_grid_2'],\n",
    "        'y_center': [sz_btm_avg + sz_third_y, sz_btm_avg + (2 * sz_third_y)],\n",
    "        'x': [-half_plate] * 2,\n",
    "        'x2': [half_plate] * 2\n",
    "    })\n",
    "    horizontal_grid = alt.Chart(horizontal_grid_data).mark_rule(\n",
    "        color='red', strokeWidth=1, opacity=0.8, strokeDash=[5, 5]\n",
    "    ).encode(y='y_center', x='x', x2='x2')\n",
    "    \n",
    "    # Strike Zones (Base and Dynamic)\n",
    "    zone_df_base = pd.DataFrame({\n",
    "        'id': ['base_zone'], 'x': [-half_plate], 'x2': [half_plate], \n",
    "        'y': [sz_btm_avg], 'y2': [sz_top_avg] \n",
    "    })\n",
    "    base_strike_zone = alt.Chart(zone_df_base).mark_rect(\n",
    "        stroke='gray', strokeWidth=2, fillOpacity=0.0\n",
    "    ).encode(x='x', x2='x2', y='y', y2='y2')\n",
    "    \n",
    "    zone_df_dynamic = pd.DataFrame({\n",
    "        'id': ['dynamic_zone'], 'x': [-half_plate], 'x2': [half_plate], \n",
    "        'y': [sz_btm_avg], 'y2': [sz_top_avg] \n",
    "    })\n",
    "    dynamic_strike_zone = alt.Chart(zone_df_dynamic).mark_rect(\n",
    "        stroke='red', strokeWidth=2, fillOpacity=0.0, strokeDash=[5, 5]\n",
    "    ).encode(x='x', x2='x2', y='y', y2='y2')\n",
    "\n",
    "    # --- B. SELECTIONS AND PITCH CHART ---\n",
    "\n",
    "    # Primary Player Selection (Batter or Pitcher)\n",
    "    player_selection = alt.selection_point(fields=[lotte_field], empty='all', name=f'{lotte_role.capitalize()}Selector', \n",
    "        bind=alt.binding_select(options=[None] + lotte_players, name=f'{lotte_role.capitalize()}:'))\n",
    "    \n",
    "    # Opponent Selection (Pitcher or Batter)\n",
    "    opponent_selection = alt.selection_point(fields=[opponent_field], empty='all', name=f'{opponent_role.capitalize()}Selector', \n",
    "        bind=alt.binding_select(options=[None] + opponent_players, name=f'{opponent_role.capitalize()}:'))\n",
    "    \n",
    "    # Pitch Type Selection\n",
    "    pitch_type_selection = alt.selection_point(fields=['pitch_type'], bind='legend')\n",
    "\n",
    "    base = alt.Chart(df_plot).add_params(player_selection, opponent_selection, pitch_type_selection)\n",
    "    \n",
    "    # Pitch Scatter Plot\n",
    "    pitch_chart = base.mark_point(filled=True, size=70).encode(\n",
    "        x=alt.X('plate_x_ft', title='Plate X (ft, Positive=RHH Outer)', axis=alt.Axis(values=[-2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5]), scale=alt.Scale(domain=[-2.5, 2.5])),\n",
    "        y=alt.Y('plate_z_ft', title='Plate Z (ft, Above Ground)', axis=alt.Axis(values=[0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5]), scale=alt.Scale(domain=[0, 5.0])),\n",
    "        color=alt.Color('plot_result', title='Action/Result').scale(domain=RESULT_DOMAIN, range=RESULT_RANGE),\n",
    "        shape=alt.Shape('pitch_type', title='Pitch Type').scale(domain=PITCH_DOMAIN, range=PITCH_RANGE),\n",
    "        \n",
    "        opacity=alt.condition(pitch_type_selection, alt.value(1.0), alt.value(0.2)),\n",
    "        \n",
    "        tooltip=['batter_name', 'pitcher_name', 'pitch_type', 'pitch_speed_kph', 'plot_result', 'final_pitch_result', 'strike', 'ball', 'out', alt.Tooltip('plate_x_ft', format='.2f'), alt.Tooltip('plate_z_ft', format='.2f'), 'pitch_id']\n",
    "    ).properties(\n",
    "        title=title_base, \n",
    "        width=WIDTH, \n",
    "        height=HEIGHT\n",
    "    ).interactive()\n",
    "\n",
    "    # Apply filters explicitly to the scatter plot layer\n",
    "    pitch_chart = pitch_chart.transform_filter(opponent_selection).transform_filter(player_selection)\n",
    "    \n",
    "    # --- C. COMBINE AND SAVE ---\n",
    "    combined_chart = (pitch_chart + base_strike_zone + dynamic_strike_zone + vertical_grid + horizontal_grid).resolve_scale(x='shared', y='shared')\n",
    "    combined_chart = combined_chart.properties(autosize=alt.AutoSizeParams(type='fit', contains='padding'))\n",
    "\n",
    "    # 1. Get the final Vega-Lite specification\n",
    "    spec = combined_chart.to_dict(format=\"vega-lite\")\n",
    "    \n",
    "    # 2. Write the dictionary to the file\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(spec, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "    print(f\"Interactive Altair chart saved to '{output_filename}'.\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Execution Block (Unchanged, ensures all files are generated) ---\n",
    "PITCH_DATA_FOLDER = 'final_processed'\n",
    "OUTPUT_BASE_PATH = 'web/public/assets'\n",
    "\n",
    "try:\n",
    "    all_files = glob.glob(os.path.join(PITCH_DATA_FOLDER, \"*.csv\"))\n",
    "    if not all_files:\n",
    "        print(f\"Error: No CSV files found in {PITCH_DATA_FOLDER}.\")\n",
    "    else:\n",
    "        df_list = [pd.read_csv(filename, index_col=None, header=0) for filename in all_files]\n",
    "        pitch_data = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "        \n",
    "        os.makedirs(OUTPUT_BASE_PATH, exist_ok=True) \n",
    "\n",
    "        # 3. GENERATE DYNAMIC STRIKE ZONE SUMMARY\n",
    "        print(\"\\nStarting Dynamic Strike Zone Summary Generation...\")\n",
    "        strikezone_df = calculate_strikezone_averages(pitch_data)\n",
    "        strikezone_output_path = os.path.join(OUTPUT_BASE_PATH, STRIKEZONE_SUMMARY_FILE)\n",
    "        strikezone_df.to_json(strikezone_output_path, orient='records', indent=4, force_ascii=False)\n",
    "        print(f\"Dynamic Strike Zone Summary exported to {STRIKEZONE_SUMMARY_FILE}.\")\n",
    "        \n",
    "        # 4. Identify all unique opposing team codes\n",
    "        opponent_teams = set(pitch_data[pitch_data['pitcher_team_code'] != LOTTE_CODE]['pitcher_team_code'].unique())\n",
    "        opponent_teams.update(pitch_data[pitch_data['batter_team_code'] != LOTTE_CODE]['batter_team_code'].unique())\n",
    "        \n",
    "        # 5. Loop and generate charts for each opponent\n",
    "        generated_files = []\n",
    "        for opponent_code in sorted(list(opponent_teams)):\n",
    "            output_file_batters = os.path.join(OUTPUT_BASE_PATH, f'{LOTTE_CODE}_batters_vs_{opponent_code}_discipline.json')\n",
    "            generate_lotte_offense_chart(pitch_data, opponent_code, output_file_batters)\n",
    "            generated_files.append(output_file_batters)\n",
    "\n",
    "            output_file_pitchers = os.path.join(OUTPUT_BASE_PATH, f'{LOTTE_CODE}_pitchers_vs_{opponent_code}_discipline.json')\n",
    "            generate_lotte_defense_chart(pitch_data, opponent_code, output_file_pitchers)\n",
    "            generated_files.append(output_file_pitchers)\n",
    "            \n",
    "        print(f\"\\nSuccessfully generated {len(generated_files)} charts and 1 strike zone summary.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f9a61-688e-48b7-8ecc-1a161c0cfa91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# more processing to label final pitch, calculate pitch tunnel data, etc and generate final_processed files for pitch-chart\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "Y_PLATE = 0\n",
    "T_EARLY = 0.150  # 150 ms\n",
    "\n",
    "# Define the directories\n",
    "INPUT_DIR = \"pitch_processed\"\n",
    "OUTPUT_DIR = \"final_processed\"\n",
    "INPUT_SUFFIX = \"_processed.csv\"\n",
    "OUTPUT_SUFFIX = \"_final.csv\"\n",
    "\n",
    "def process_pitch_data(file_path: str, output_dir: str, season_type: str, season_year: str) -> None:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {file_path}: Error loading data: {e}\")\n",
    "        return\n",
    "    \n",
    "    game_id = filename.split('_')[0]\n",
    "\n",
    "    df['game_id'] = game_id\n",
    "    df['season_year'] = season_year\n",
    "    df['season_type'] = season_type\n",
    "\n",
    "    has_pitch_data = 'ball_count' in df.columns and 'x0' in df.columns\n",
    "\n",
    "    # 1. Convert Base IDs to binary (1 if someone is there, 0 if not)\n",
    "    for col in ['base1', 'base2', 'base3']:\n",
    "        df[f'{col}_occupied'] = (df[col] > 0).astype(int)\n",
    "    \n",
    "    # 2. Create a 'base_state' string (e.g., \"100\" for runner on 1st, \"111\" for loaded)\n",
    "    df['base_state'] = (\n",
    "        df['base1_occupied'].astype(str) + \n",
    "        df['base2_occupied'].astype(str) + \n",
    "        df['base3_occupied'].astype(str)\n",
    "    )\n",
    "    \n",
    "    # 3. Create the unique 24-state key\n",
    "    df['re_state'] = df['out'].astype(str) + \" out, \" + df['base_state']\n",
    "    \n",
    "    if not has_pitch_data:\n",
    "        print(f\"  (!) No pitch tracking data found in {os.path.basename(file_path)}. Skipping tunnel calcs.\")\n",
    "    else:\n",
    "        # --- 1. Identify the Unique Plate Appearance (PA) ---\n",
    "        df['pa_id'] = (\n",
    "            df['batter_team_code'] + '_' + \n",
    "            df['inning'].astype(str) + '_' + \n",
    "            df['batter_id'].astype(str)\n",
    "        )\n",
    "    \n",
    "        # --- 2. Identify the Final Pitch in Each PA ---\n",
    "        df['final_pitch_id'] = df.groupby('pa_id')['pitch_id'].transform('max')\n",
    "        df['is_final_pitch'] = df['pitch_id'] == df['final_pitch_id']\n",
    "    \n",
    "        # --- 3. Create the Conditional Outcome Column with Edge Case Handling ---\n",
    "    \n",
    "        # Check 1: Runner Out/PA Ended case (N/A in pa_result_long)\n",
    "        is_runner_out_case = (\n",
    "            df['is_final_pitch'] == True\n",
    "        ) & (\n",
    "            df['pa_result_long'].astype(str).str.upper().str.strip() == 'N/A'\n",
    "        )\n",
    "    \n",
    "        # Check 2: Standard PA result case (Not N/A)\n",
    "        is_standard_result_case = (\n",
    "            df['is_final_pitch'] == True\n",
    "        ) & (\n",
    "            df['pa_result_long'].astype(str).str.upper().str.strip() != 'N/A'\n",
    "        )\n",
    "    \n",
    "        # Define the conditions and choices for numpy.select\n",
    "        conditions = [\n",
    "            is_runner_out_case,         # Final pitch AND PA result is N/A\n",
    "            is_standard_result_case     # Final pitch AND PA result is NOT N/A\n",
    "        ]\n",
    "    \n",
    "        choices = [\n",
    "            'Runner Out/PA Ended',      # Choice 1: Clear marker for non-batter result\n",
    "            df['pa_result_short']       # Choice 2: Use the short PA result\n",
    "        ]\n",
    "    \n",
    "        # Default choice: In-at-bat pitch result\n",
    "        default_choice = df['pitch_result']\n",
    "    \n",
    "        df['final_pitch_result'] = np.select(\n",
    "            conditions, \n",
    "            choices, \n",
    "            default=default_choice\n",
    "        )\n",
    "        \n",
    "        # --- 4. Calculate ball location 150ms before plate ---\n",
    "        \n",
    "        # 1. Calculate Time to Plate (T_plate)\n",
    "        df['t_plate'] = df.apply(solve_time_to_plate, axis=1)\n",
    "    \n",
    "        # 2. Calculate the Time for the 150ms (t_150ms) tunnel point\n",
    "        df['t_150ms'] = df['t_plate'] - T_EARLY\n",
    "        \n",
    "        # Ensure t_150ms is positive (meaning the pitch lasts at least 150ms)\n",
    "        df.loc[df['t_150ms'] <= 0, 't_150ms'] = np.nan\n",
    "    \n",
    "        # 3. Calculate X and Z Coordinates at t_150ms\n",
    "        # Apply the position calculation only for pitches where t_150ms is valid\n",
    "        valid_mask = df['t_150ms'].notna()\n",
    "        if valid_mask.any():\n",
    "            results = df.loc[valid_mask].apply(\n",
    "                lambda row: calculate_position_at_time(row, row['t_150ms']), \n",
    "                axis=1, result_type='expand'\n",
    "            )\n",
    "            # Rename columns and merge back into the main DataFrame\n",
    "            results.columns = ['x_150ms', 'y_150ms', 'z_150ms']\n",
    "            df = df.merge(results, left_index=True, right_index=True, how='left')\n",
    "        \n",
    "        # 4. Round the new columns to 4 decimal places for clean storage\n",
    "        for col in ['x_150ms', 'y_150ms', 'z_150ms', 't_plate', 't_150ms']:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].round(4)\n",
    "    \n",
    "    # --- 5. Save the Updated Data ---\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    base_name = os.path.basename(file_path).replace(INPUT_SUFFIX, OUTPUT_SUFFIX)\n",
    "    output_path = os.path.join(output_dir, base_name)\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8-sig') \n",
    "\n",
    "\n",
    "# Calculates the 3D position of the ball at time t using constant acceleration kinematics.\n",
    "def calculate_position_at_time(row, t: float) -> tuple[float, float, float]:\n",
    "    x = row['x0'] + row['vx0'] * t + 0.5 * row['ax'] * (t**2)\n",
    "    y = row['y0'] + row['vy0'] * t + 0.5 * row['ay'] * (t**2)\n",
    "    z = row['z0'] + row['vz0'] * t + 0.5 * row['az'] * (t**2)  \n",
    "    return x, y, z\n",
    "\n",
    "\n",
    "def solve_time_to_plate(row):\n",
    "    a = 0.5 * row['ay']\n",
    "    b = row['vy0']\n",
    "    c = row['y0'] - Y_PLATE\n",
    "    \n",
    "    discriminant = b**2 - 4 * a * c\n",
    "    \n",
    "    if discriminant < 0:\n",
    "        return np.nan\n",
    "    \n",
    "    sqrt_discriminant = np.sqrt(discriminant)\n",
    "    \n",
    "    # Two possible roots: root1 and root2\n",
    "    root1 = (-b + sqrt_discriminant) / (2 * a)\n",
    "    root2 = (-b - sqrt_discriminant) / (2 * a)\n",
    "    \n",
    "    # The correct time is the positive root (time in flight)\n",
    "    roots = [r for r in [root1, root2] if r > 0]\n",
    "    return min(roots) if roots else np.nan\n",
    "    \n",
    "\n",
    "# --- Batch Processing Execution ---\n",
    "sub_folders = {\n",
    "    \"\": \"reg\",      # Root of INPUT_DIR is considered regular season\n",
    "    \"post\": \"post\",\n",
    "    \"pre\": \"pre\"\n",
    "}\n",
    "\n",
    "print(\"Starting batch processing...\")\n",
    "\n",
    "# os.walk moves through every subfolder automatically\n",
    "for root, dirs, files in os.walk(INPUT_DIR):\n",
    "    dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "    csv_files = [f for f in files if f.endswith(INPUT_SUFFIX) and not f.startswith('.')]\n",
    "    \n",
    "    if not csv_files:\n",
    "        continue\n",
    "\n",
    "    # 1. Determine relative path from input base (e.g., '2025/pre')\n",
    "    rel_path = os.path.relpath(root, INPUT_DIR)\n",
    "\n",
    "    path_parts = rel_path.split(os.sep)\n",
    "    s_year = path_parts[0]\n",
    "    \n",
    "    # 2. Determine Season Type based on folder name\n",
    "    # If the current folder is 'pre' or 'post', use that. Otherwise, it's 'reg'.\n",
    "    folder_name = os.path.basename(root)\n",
    "    if folder_name in ['pre', 'post']:\n",
    "        s_type = folder_name\n",
    "    else:\n",
    "        s_type = 'reg'\n",
    "\n",
    "    # 3. Create corresponding output directory\n",
    "    current_output_dir = os.path.join(OUTPUT_DIR, rel_path)\n",
    "\n",
    "    for filename in csv_files:\n",
    "        file_path = os.path.join(root, filename)\n",
    "\n",
    "        process_pitch_data(file_path, current_output_dir, s_type, s_year)\n",
    "        print(f\"Processed: {current_output_dir}/{file_path} ({s_type}, {s_year})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2626b2e-a860-4f48-9785-69e76e7faac4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# generate pitcher_effectiveness_season_summary.json for pitcher-stats\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import common as cmn\n",
    "\n",
    "# Define the folders for batch processing\n",
    "INPUT_DIR = \"pitch_processed\"\n",
    "OUTPUT_DIR = 'web/public/assets'\n",
    "OUTPUT_FILE = \"pitcher_effectiveness_season_summary.json\"\n",
    "\n",
    "def calculate_pitcher_effectiveness(df: pd.DataFrame, is_all_teams: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates essential pitch rates, including the new Swing Rate, grouped by \n",
    "    Pitcher, Pitch Type, Pitcher Team, Opponent Batter Team, and Windup/Stretch.\n",
    "    \"\"\"\n",
    "    # Define Counting Variables (assuming 'S', 'V', 'F', 'H' are the relevant codes)\n",
    "    df['is_swing'] = df['pitch_result'].isin(list(cmn.SWING_CODES)).astype(int)\n",
    "    #df['is_whiff'] = df['pitch_result'].isin(['S', 'V']).astype(int)\n",
    "    df['is_contact'] = df['pitch_result'].isin(list(cmn.CONTACT_CODES)).astype(int) \n",
    "\n",
    "    # Define the core grouping keys\n",
    "    grouping_keys = [\n",
    "        'pitcher_team_code',\n",
    "        'pitcher_name', \n",
    "        'pitch_type', \n",
    "        'is_throwing_stretch'\n",
    "    ]\n",
    "\n",
    "    if not is_all_teams:\n",
    "        grouping_keys.insert(1, 'batter_team_code')\n",
    "    \n",
    "    # Group and Aggregate Counts\n",
    "    pitcher_effectiveness = df.groupby(grouping_keys).agg(\n",
    "        total_pitches=('pitch_id', 'count'),\n",
    "        total_swings=('is_swing', 'sum'),\n",
    "        contacts=('is_contact', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    if is_all_teams:\n",
    "        pitcher_effectiveness.insert(1, 'batter_team_code', 'ALL')\n",
    "    \n",
    "    # Calculate Rates (Handle Division by Zero)\n",
    "    \n",
    "    # 1. Swing Rate (Swings / Total Pitches)\n",
    "    pitcher_effectiveness['swing_rate'] = np.where(\n",
    "        pitcher_effectiveness['total_pitches'] > 0,\n",
    "        pitcher_effectiveness['total_swings'] / pitcher_effectiveness['total_pitches'],\n",
    "        np.nan \n",
    "    )\n",
    "\n",
    "    # 2. Contact Rate (Contacts / Total Swings)\n",
    "    pitcher_effectiveness['contact_rate'] = np.where(\n",
    "        pitcher_effectiveness['total_swings'] > 0,\n",
    "        pitcher_effectiveness['contacts'] / pitcher_effectiveness['total_swings'],\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    return pitcher_effectiveness\n",
    "\n",
    "def load_and_process_season_data(input_dir):\n",
    "    \"\"\"Loads all CSV files from a directory and runs the effectiveness calculation.\"\"\"\n",
    "    input_files = glob.glob(os.path.join(input_dir, '*.csv'))\n",
    "    if not input_files:\n",
    "        print(f\"Error: No CSV files found in the directory: {input_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_files = []\n",
    "    print(f\"Found {len(input_files)} files. Combining data...\")\n",
    "    for file in input_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            all_files.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping file {file} due to error: {e}\")\n",
    "\n",
    "    if not all_files:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    return pd.concat(all_files, ignore_index=True)\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# Define the focus team code\n",
    "LOTTE_TEAM_CODE = 'LT'\n",
    "ALL_CODE = 'ALL'\n",
    "\n",
    "# 1. Load and calculate the initial metrics\n",
    "raw_combined_df = load_and_process_season_data(INPUT_DIR)\n",
    "\n",
    "if raw_combined_df.empty:\n",
    "    print(\"Processing failed or no data was loaded.\")\n",
    "else:\n",
    "    # 2a. Calculate Team-vs-Team (Includes all specific matchups, e.g., LT vs. HH)\n",
    "    print(\"\\nCalculating Team-vs-Team metrics...\")\n",
    "    team_vs_team_df = calculate_pitcher_effectiveness(raw_combined_df, is_all_teams=False)\n",
    "    \n",
    "    print(\"Calculating LT Pitchers vs. ALL Batters metrics...\")\n",
    "    lt_pitchers_df = raw_combined_df[raw_combined_df['pitcher_team_code'] == LOTTE_TEAM_CODE].copy()\n",
    "    lt_vs_all_batters_df = calculate_pitcher_effectiveness(lt_pitchers_df, is_all_teams=True)\n",
    "    \n",
    "    # 2b. Concatenate the results: Team-vs-Team + LT vs. ALL Batters\n",
    "    long_format_df = pd.concat([team_vs_team_df, lt_vs_all_batters_df], ignore_index=True)\n",
    "    \n",
    "    # 3. Prepare for Pivoting\n",
    "    long_format_df['stretch_status'] = np.where(\n",
    "        long_format_df['is_throwing_stretch'] == True, \n",
    "        'stretch', \n",
    "        'windup'\n",
    "    )\n",
    "    \n",
    "    # 4. Define the columns to pivot\n",
    "    value_cols = [\n",
    "        'total_pitches', \n",
    "        'swing_rate', \n",
    "        'contact_rate'\n",
    "    ]\n",
    "    \n",
    "    id_vars = [\n",
    "        'pitcher_team_code', 'batter_team_code', 'pitcher_name', 'pitch_type'\n",
    "    ]\n",
    "\n",
    "    # 5. Pivot the DataFrame to the requested wide format\n",
    "    wide_format_df = long_format_df.pivot(\n",
    "        index=id_vars,\n",
    "        columns='stretch_status',\n",
    "        values=value_cols\n",
    "    ).reset_index()\n",
    "\n",
    "    # 6. Flatten the column multi-index and rename\n",
    "    wide_format_df.columns = [\n",
    "        '_'.join(map(str, col)).strip('_') \n",
    "        if col[0] not in id_vars else col[0] \n",
    "        for col in wide_format_df.columns.values\n",
    "    ]\n",
    "    \n",
    "    # 7. Define the Final Column Order and Clean Names\n",
    "    final_order_cols = [\n",
    "        'pitcher_team_code',\n",
    "        'batter_team_code',\n",
    "        'pitcher_name',\n",
    "        'pitch_type',\n",
    "        'total_pitches_windup',\n",
    "        'swing_rate_windup',\n",
    "        'contact_rate_windup',\n",
    "        'total_pitches_stretch',\n",
    "        'swing_rate_stretch',\n",
    "        'contact_rate_stretch',\n",
    "    ]\n",
    "    \n",
    "    # Select and rename columns for final output\n",
    "    final_df = wide_format_df.reindex(columns=final_order_cols).rename(columns={\n",
    "        'total_pitches_windup': 'pitches (windup)',\n",
    "        'swing_rate_windup': 'swing rate (windup)',\n",
    "        'contact_rate_windup': 'contact rate (windup)',\n",
    "        \n",
    "        'total_pitches_stretch': 'pitches (stretch)',\n",
    "        'swing_rate_stretch': 'swing rate (stretch)',\n",
    "        'contact_rate_stretch': 'contact rate (stretch)',\n",
    "    })\n",
    "    \n",
    "    # 7. Save the final DataFrame to JSON\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Save the file\n",
    "    final_df.to_json(f'{OUTPUT_DIR}/{OUTPUT_FILE}', orient='records', indent=4, force_ascii=False)\n",
    "    print(f\"\\n✅ Final streamlined season data successfully exported to {OUTPUT_DIR}/{OUTPUT_FILE}.\")\n",
    "    print(\"You can now safely update your Angular interface and HTML template to use the new column names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2472cf99-1463-42dc-b4bd-f35afb1887a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# double check all pitch_result are known to me\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from typing import Set\n",
    "import common as cmn\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_DIR = \"pitch_processed\"\n",
    "\n",
    "# Your currently known pitch result codes (keys of your map)\n",
    "KNOWN_PITCH_CODES = set(cmn.PITCH_RESULT_MAP.keys())\n",
    "# ---------------------\n",
    "\n",
    "def validate_pitch_results(input_dir: str, known_codes: Set[str]) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Scans all CSV files in the input directory, extracts all unique \n",
    "    'pitch_result' codes, and returns any codes not found in the \n",
    "    KNOWN_PITCH_CODES set.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: The path to the folder containing the CSV files.\n",
    "        known_codes: A set of expected pitch result codes.\n",
    "        \n",
    "    Returns:\n",
    "        A set of unknown pitch result codes found in the data.\n",
    "    \"\"\"\n",
    "    # 1. Prepare file list\n",
    "    search_path = os.path.join(input_dir, '*.csv')\n",
    "    input_files = glob.glob(search_path)\n",
    "    \n",
    "    if not input_files:\n",
    "        print(f\"⚠️ Warning: No CSV files found in directory: '{input_dir}'\")\n",
    "        return set()\n",
    "\n",
    "    print(f\"Found {len(input_files)} CSV files. Starting validation...\")\n",
    "    \n",
    "    # 2. Initialize a set to store all unique codes found in the data\n",
    "    all_codes_found = set()\n",
    "    \n",
    "    # 3. Iterate through files and extract unique codes\n",
    "    for i, file_path in enumerate(input_files):\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        try:\n",
    "            # Read only the 'pitch_result' column, optimizing memory use\n",
    "            df = pd.read_csv(file_path, usecols=['pitch_result'])\n",
    "            \n",
    "            # Update the set with all unique codes from the current file\n",
    "            all_codes_found.update(df['pitch_result'].astype(str).unique())\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"    Error: File not found at {file_path}\")\n",
    "        except ValueError:\n",
    "            print(f\"    Error: 'pitch_result' column not found in {file_name}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    An unexpected error occurred while processing {file_name}: {e}\")\n",
    "\n",
    "    # 4. Determine unknown codes\n",
    "    # This is the set difference: (all codes found) - (known codes)\n",
    "    unknown_codes = all_codes_found - known_codes\n",
    "    \n",
    "    return unknown_codes\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unknown_codes_set = validate_pitch_results(INPUT_DIR, KNOWN_PITCH_CODES)\n",
    "    \n",
    "    print(\"\\n--- Validation Summary ---\")\n",
    "    \n",
    "    if unknown_codes_set:\n",
    "        print(\"🔴 UNKNOWN PITCH RESULT CODES FOUND:\")\n",
    "        # Display the unknown codes sorted alphabetically\n",
    "        for code in sorted(list(unknown_codes_set)):\n",
    "            print(f\"  - '{code}'\")\n",
    "        \n",
    "        print(\"\\nACTION REQUIRED: Please define these codes in your PITCH_RESULT_MAP.\")\n",
    "    else:\n",
    "        print(f\"✅ Success! All 'pitch_result' codes match your known set of {len(KNOWN_PITCH_CODES)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fbac7a-08bd-42a8-9f6c-a793439abc3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# count pitch types frequency\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from typing import Dict, Any\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_DIR = \"pitch_processed\"\n",
    "TARGET_COLUMN = \"pitch_type\"\n",
    "# ---------------------\n",
    "\n",
    "def count_column_frequencies(input_dir: str, column_name: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Scans all CSV files in the input directory and calculates the total \n",
    "    frequency (count) for every unique value in the specified column.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: The path to the folder containing the CSV files.\n",
    "        column_name: The name of the column to inspect (e.g., 'pitch_type').\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary mapping each unique value (str) to its total count (int).\n",
    "    \"\"\"\n",
    "    # 1. Prepare file list\n",
    "    search_path = os.path.join(input_dir, '*.csv')\n",
    "    input_files = glob.glob(search_path)\n",
    "    \n",
    "    if not input_files:\n",
    "        print(f\"⚠️ Warning: No CSV files found in directory: '{input_dir}'\")\n",
    "        return {}\n",
    "\n",
    "    print(f\"Found {len(input_files)} CSV files. Starting frequency counting for '{column_name}'...\")\n",
    "    \n",
    "    # 2. Initialize a Series to accumulate counts (will be converted to dict at end)\n",
    "    # Using a list to store Series results is memory-efficient for large numbers of files\n",
    "    all_counts = []\n",
    "    \n",
    "    # 3. Iterate through files and collect counts\n",
    "    for i, file_path in enumerate(input_files):\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        try:\n",
    "            # Read only the target column\n",
    "            df = pd.read_csv(file_path, usecols=[column_name])\n",
    "            \n",
    "            # Calculate counts for the current file and append to list\n",
    "            counts = df[column_name].astype(str).value_counts()\n",
    "            all_counts.append(counts)\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"    Error: File not found at {file_path}\")\n",
    "        except ValueError:\n",
    "            print(f\"    Error: Column '{column_name}' not found in {file_name}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    An unexpected error occurred while processing {file_name}: {e}\")\n",
    "\n",
    "    # 4. Combine and summarize the counts\n",
    "    if not all_counts:\n",
    "        return {}\n",
    "\n",
    "    # Concatenate all individual Series of counts into one large Series\n",
    "    combined_counts = pd.concat(all_counts)\n",
    "    \n",
    "    # Group by the index (pitch type) and sum the counts\n",
    "    final_frequencies = combined_counts.groupby(combined_counts.index).sum()\n",
    "    \n",
    "    # Convert the final Series to a standard dictionary for output\n",
    "    return final_frequencies.to_dict()\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pitch_frequencies = count_column_frequencies(INPUT_DIR, TARGET_COLUMN)\n",
    "    \n",
    "    total_pitches = sum(pitch_frequencies.values())\n",
    "    \n",
    "    print(\"\\n--- Pitch Type Frequency Summary ---\")\n",
    "    \n",
    "    if pitch_frequencies:\n",
    "        print(f\"Total Pitches Counted: {total_pitches:,}\")\n",
    "        print(\"\\nDistribution of Pitch Types (Sorted by Count, descending):\")\n",
    "        \n",
    "        # Sort the dictionary items by count (value) in descending order\n",
    "        sorted_frequencies = sorted(pitch_frequencies.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "        for pitch_type, count in sorted_frequencies:\n",
    "            # Calculate percentage for clearer analysis\n",
    "            percentage = (count / total_pitches) * 100 if total_pitches else 0\n",
    "            print(f\"  - {pitch_type:<10}: {count:>8,} ({percentage:.2f}%)\")\n",
    "            \n",
    "    else:\n",
    "        print(\"❌ Could not count any pitch types. Check the folder path and file contents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005ee91-a505-4ceb-8644-96e4dc42198b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate pitch tunneling json data all_pitches.json\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_DIR = \"pitch_processed\"\n",
    "OUTPUT_DIR = 'web/public/assets'\n",
    "OUTPUT_FILE = \"all_pitches.json\"\n",
    "\n",
    "# Define the columns required for the physics model and metadata\n",
    "REQUIRED_COLUMNS = [\n",
    "    'pitch_id', 'pitcher_team_code', 'pitcher_name', 'pitch_type', 'is_throwing_stretch', \n",
    "    'x0', 'z0', 'y0', # release point (ft)\n",
    "    'vx0', 'vy0', 'vz0', # initial velocity components (ft/s)\n",
    "    'ax', 'ay', 'az' # acceleration (ft/s²)\n",
    "]\n",
    "# Critical columns that MUST have valid numeric data for the analysis to work\n",
    "CRITICAL_SUBSET = ['x0', 'z0', 'vx0', 'vy0', 'ax', 'az']\n",
    "# ---------------------\n",
    "\n",
    "def process_single_file(file_path: str, required_cols: List[str], critical_subset: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Reads, cleans, and processes a single CSV file, returning a list of clean pitch dictionaries.\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    try:\n",
    "        # Read the CSV. \n",
    "        # Low_memory=False helps pandas handle potentially inconsistent dtypes across many columns\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"    Error: File not found at {file_name}. Skipping.\")\n",
    "        return []\n",
    "\n",
    "    # Check if all required columns are present\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"    Warning: Missing required columns in {file_name}: {', '.join(missing_cols)}. Skipping.\")\n",
    "        return []\n",
    "\n",
    "    # 1. Filter only the required columns\n",
    "    df_clean = df[required_cols].copy()\n",
    "\n",
    "    # 2. Convert key columns to numeric types\n",
    "    numeric_cols = [col for col in required_cols if col not in ['pitch_id', 'pitcher_team_code', 'pitcher_name', 'pitch_type']]\n",
    "    for col in numeric_cols:\n",
    "        # The 'errors='coerce'' argument turns any non-numeric data into NaN.\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "\n",
    "    # 3. Handle missing data: Drop rows where any critical calculation column is missing\n",
    "    initial_count = len(df_clean)\n",
    "    df_clean.dropna(subset=critical_subset, inplace=True)\n",
    "    \n",
    "    pitches_dropped = initial_count - len(df_clean)\n",
    "    if pitches_dropped > 0:\n",
    "        print(f\"    Cleaned: Dropped {pitches_dropped} rows with missing critical data.\")\n",
    "\n",
    "    # 4. Convert boolean flag\n",
    "    if 'is_throwing_stretch' in df_clean.columns:\n",
    "        df_clean['is_throwing_stretch'] = df_clean['is_throwing_stretch'].astype(bool)\n",
    "\n",
    "    # 5. Convert the clean DataFrame to a list of Python dictionaries\n",
    "    return df_clean.to_dict('records')\n",
    "\n",
    "\n",
    "def aggregate_and_save_data(input_dir: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Main function to scan the directory, process all CSVs, aggregate, and save.\n",
    "    \"\"\"\n",
    "    # Use glob to find all CSV files recursively if needed (using **), but usually just *.csv is enough\n",
    "    search_path = os.path.join(input_dir, '*.csv')\n",
    "    input_files = glob.glob(search_path)\n",
    "    \n",
    "    if not input_files:\n",
    "        print(f\"❌ Error: No CSV files found in the directory: '{input_dir}'.\")\n",
    "        print(\"Please ensure your CSV files are placed inside this folder.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(input_files)} CSV files in '{input_dir}'. Starting aggregation...\")\n",
    "    \n",
    "    all_pitches = []\n",
    "    \n",
    "    for i, file_path in enumerate(input_files):\n",
    "        print(f\"Processing file {i+1}/{len(input_files)}: {os.path.basename(file_path)}\")\n",
    "        clean_pitches = process_single_file(file_path, REQUIRED_COLUMNS, CRITICAL_SUBSET)\n",
    "        all_pitches.extend(clean_pitches)\n",
    "        \n",
    "    final_count = len(all_pitches)\n",
    "    \n",
    "    if final_count == 0:\n",
    "        print(\"\\n⚠️ No clean pitch data was aggregated. Check your CSV files for content and column headers.\")\n",
    "        return\n",
    "\n",
    "    # Save the aggregated data to a single JSON file\n",
    "    print(f\"\\n✅ Aggregation complete. Total clean pitches: {final_count:,}\")\n",
    "    print(f\"Saving {final_count:,} records to {output_path}\")\n",
    "\n",
    "    # Use indent=4 for human-readable output (helpful for verification)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_pitches, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    print(f\"Data ready! You now have a single JSON file: {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    aggregate_and_save_data(INPUT_DIR, f'{OUTPUT_DIR}/{OUTPUT_FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6090330c-79d3-4aa7-a7fa-123cbe18849c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CHATGPT: compute release + 150ms + plate centroids and pairwise tunnels\n",
    "import os, glob, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "FOLDER = \"pitch_processed\"\n",
    "OUTPUT_DIR = 'web/public/assets'\n",
    "OUT_JSON = \"pitch_tunnel_pairs.json\"\n",
    "OUT_CENTROIDS = \"tunnel_centroids_stages.csv\"\n",
    "T_EARLY = 0.150  # 150 ms\n",
    "\n",
    "# helper: valid row check\n",
    "def valid_row(r):\n",
    "    required = ['x0','y0','z0','vx0','vy0','vz0','ax','ay','az','plate_x_ft','plate_z_ft','pitch_type','pitcher_id']\n",
    "    for c in required:\n",
    "        if c not in r or pd.isna(r[c]) or r[c] == '' or r[c] == -1:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# compute position at time t using p(t)=p0 + v0*t + 0.5*a*t^2\n",
    "def pos_at_t(row, t):\n",
    "    x = row['x0'] + row['vx0'] * t + 0.5 * row['ax'] * t**2\n",
    "    y = row['y0'] + row['vy0'] * t + 0.5 * row['ay'] * t**2\n",
    "    z = row['z0'] + row['vz0'] * t + 0.5 * row['az'] * t**2\n",
    "    return np.array([x, y, z])\n",
    "\n",
    "# load & concat\n",
    "files = glob.glob(os.path.join(FOLDER, \"*.csv\"))\n",
    "if not files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {FOLDER}/\")\n",
    "\n",
    "dfs = []\n",
    "for f in files:\n",
    "    d = pd.read_csv(f)\n",
    "    dfs.append(d)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Loaded {len(files)} files, total rows: {len(df)}\")\n",
    "\n",
    "# ensure numeric where needed\n",
    "num_cols = ['x0','y0','z0','vx0','vy0','vz0','ax','ay','az','plate_x_ft','plate_z_ft']\n",
    "for c in num_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "df['pitch_type'] = df['pitch_type'].astype(str)\n",
    "df['pitcher_id'] = df['pitcher_id'].astype(str)\n",
    "\n",
    "# filter valid rows and compute pos150 and plate vector\n",
    "records = []\n",
    "skipped = 0\n",
    "for _, row in df.iterrows():\n",
    "    if not valid_row(row):\n",
    "        skipped += 1\n",
    "        continue\n",
    "    try:\n",
    "        release = np.array([row['x0'], row['y0'], row['z0']])\n",
    "        pos150 = pos_at_t(row, T_EARLY)\n",
    "        plate = np.array([row['plate_x_ft'], row['y0'] + 18.44 if pd.isna(row['plate_x_ft']) else row['plate_x_ft'], row['plate_z_ft']])\n",
    "        # NOTE: we keep plate as (x, y_estimate, z) but pairwise distances are computed in XZ plane (horizontal+vertical)\n",
    "        records.append({\n",
    "            'pitcher_id': row['pitcher_id'],\n",
    "            'pitcher_team_code': str(row.get('pitcher_team_code', '')),\n",
    "            'pitcher_name': str(row.get('pitcher_name', '')),\n",
    "            'pitch_type': row['pitch_type'],\n",
    "            'release_x': float(release[0]), 'release_y': float(release[1]), 'release_z': float(release[2]),\n",
    "            'early_x': float(pos150[0]), 'early_y': float(pos150[1]), 'early_z': float(pos150[2]),\n",
    "            'plate_x': float(row['plate_x_ft']), 'plate_z': float(row['plate_z_ft'])\n",
    "        })\n",
    "    except Exception:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "print(f\"Valid pitch rows: {len(records)}  Skipped rows: {skipped}\")\n",
    "\n",
    "# build DataFrame\n",
    "rp = pd.DataFrame(records)\n",
    "\n",
    "# We'll compute centroids for each pitcher -> pitch_type -> stage\n",
    "centroids = []\n",
    "for pitcher, g in rp.groupby('pitcher_id'):\n",
    "    for ptype, pg in g.groupby('pitch_type'):\n",
    "        # compute mean positions\n",
    "        mean_release_x = pg['release_x'].mean()\n",
    "        mean_release_z = pg['release_z'].mean()\n",
    "        mean_early_x = pg['early_x'].mean()\n",
    "        mean_early_z = pg['early_z'].mean()\n",
    "        mean_plate_x = pg['plate_x'].mean()\n",
    "        mean_plate_z = pg['plate_z'].mean()\n",
    "        count = len(pg)\n",
    "        centroids.append({\n",
    "            'pitcher_id': pitcher,\n",
    "            'pitcher_team_code': pg['pitcher_team_code'].iloc[0],\n",
    "            'pitcher_name': pg['pitcher_name'].iloc[0],\n",
    "            'pitch_type': ptype,\n",
    "            'count': int(count),\n",
    "            \n",
    "            'release_x': float(mean_release_x),\n",
    "            'release_z': float(mean_release_z),\n",
    "            'early_x': float(mean_early_x),\n",
    "            'early_z': float(mean_early_z),\n",
    "            'plate_x': float(mean_plate_x),\n",
    "            'plate_z': float(mean_plate_z)\n",
    "        })\n",
    "\n",
    "cent_df = pd.DataFrame(centroids)\n",
    "\n",
    "# save centroids CSV for sanity checks / frontend use\n",
    "cent_df.to_csv(f'{OUTPUT_DIR}/{OUT_CENTROIDS}', index=False)\n",
    "print(f\"Wrote centroids for {len(cent_df)} pitcher/pitch_type combos → {OUT_CENTROIDS}\")\n",
    "\n",
    "# compute pairwise distances (XZ-plane) per pitcher between all pitch-type combos\n",
    "result = {}\n",
    "for pitcher, g in cent_df.groupby('pitcher_id'):\n",
    "    combos = []\n",
    "    types = g['pitch_type'].tolist()\n",
    "    # iterate unique unordered pairs\n",
    "    for i in range(len(g)):\n",
    "        for j in range(i+1, len(g)):\n",
    "            a = g.iloc[i]\n",
    "            b = g.iloc[j]\n",
    "            # consider XZ-plane distances (horizontal + vertical at early & plate & release)\n",
    "            d_release = float(np.linalg.norm(np.array([a['release_x'], a['release_z']]) - np.array([b['release_x'], b['release_z']])))\n",
    "            d_early   = float(np.linalg.norm(np.array([a['early_x'],   a['early_z']])   - np.array([b['early_x'],   b['early_z']])))\n",
    "            d_late    = float(np.linalg.norm(np.array([a['plate_x'],   a['plate_z']])   - np.array([b['plate_x'],   b['plate_z']])))\n",
    "            tunnel_ratio = d_early / d_late if d_late != 0 else None\n",
    "            combos.append({\n",
    "                'pair': f\"{a['pitch_type']}-{b['pitch_type']}\",\n",
    "                'count_A': int(a['count']),\n",
    "                'count_B': int(b['count']),\n",
    "                'release_dist_ft': d_release,\n",
    "                'early_dist_ft': d_early,\n",
    "                'late_dist_ft': d_late,\n",
    "                'tunnel_ratio': tunnel_ratio\n",
    "            })\n",
    "            result[pitcher] = {\n",
    "                \"pitcher_team_code\": g['pitcher_team_code'].iloc[0],\n",
    "                \"pitcher_name\": g['pitcher_name'].iloc[0],\n",
    "                \"pairs\": combos\n",
    "            }\n",
    "\n",
    "# write JSON output\n",
    "with open(f'{OUTPUT_DIR}/{OUT_JSON}', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Wrote pairwise tunnel metrics for {len(result)} pitchers → {OUT_JSON}\")\n",
    "\n",
    "# quick top summary print for the first few pitchers\n",
    "for pid, pdata in list(result.items())[:5]:\n",
    "    print(f\"\\nPitcher {pid} {pdata['pitcher_team_code']} {pdata['pitcher_name']} - top pairs (sorted by early_dist asc):\")\n",
    "    \n",
    "    # the actual list of pair metrics\n",
    "    pairs = pdata[\"pairs\"]\n",
    "\n",
    "    combos_sorted = sorted(pairs, key=lambda x: x['early_dist_ft'])\n",
    "    \n",
    "    for c in combos_sorted[:6]:\n",
    "        print(f\"  {c['pair']}: early={c['early_dist_ft']:.3f}ft \"\n",
    "3              f\"late={c['late_dist_ft']:.3f}ft ratio={c['tunnel_ratio']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c0f69f-26ad-4819-b383-96402dbce222",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# pitch sequence tunnel (tunnel_sequence_analysis.json)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "def aggregate_tunnel_data(input_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads all processed files, calculates the 150ms tunnel distance for pitch pairs,\n",
    "    and aggregates the results.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: The directory containing the CSV files with x_150ms and z_150ms data.\n",
    "        \n",
    "    Returns:\n",
    "        A DataFrame containing the aggregated tunnel metrics for each pitch pair.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    input_files = glob.glob(os.path.join(input_dir, '*_with_final_result.csv'))\n",
    "    \n",
    "    if not input_files:\n",
    "        print(f\"No processed files found in {input_dir}. Please run the previous script first.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # --- 1. Load and Concatenate Data ---\n",
    "    for file in input_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_data:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    master_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Ensure necessary columns are present and clean (drop rows where tunnel point was invalid)\n",
    "    master_df = master_df.dropna(subset=['pa_id', 'pitch_id', 'x_150ms', 'z_150ms'])\n",
    "    master_df = master_df.sort_values(['pa_id', 'pitch_id'])\n",
    "\n",
    "    # --- 2. Create Pitch Pair Sequences ---\n",
    "\n",
    "    # Get the 150ms coordinates of the *next* pitch in the sequence\n",
    "    # Use .shift(-1) within each PA group to align pitch 1 with the coordinates of pitch 2\n",
    "    master_df['next_x_150ms'] = master_df.groupby('pa_id')['x_150ms'].shift(-1)\n",
    "    master_df['next_z_150ms'] = master_df.groupby('pa_id')['z_150ms'].shift(-1)\n",
    "    master_df['next_pitch_type'] = master_df.groupby('pa_id')['pitch_type'].shift(-1)\n",
    "\n",
    "    # Filter out the last pitch of each plate appearance (which has no 'next' pitch)\n",
    "    pitch_pairs_df = master_df.dropna(subset=['next_x_150ms']).copy()\n",
    "    \n",
    "    # Create the descriptive pair identifier\n",
    "    pitch_pairs_df['pitch_pair'] = pitch_pairs_df['pitch_type'] + ' -> ' + pitch_pairs_df['next_pitch_type']\n",
    "    \n",
    "    # --- 3. Calculate Tunnel Distance ---\n",
    "    \n",
    "    # Euclidean distance in the x-z plane (Batter POV) at the 150ms point:\n",
    "    # distance = sqrt((x2 - x1)^2 + (z2 - z1)^2)\n",
    "    delta_x = pitch_pairs_df['next_x_150ms'] - pitch_pairs_df['x_150ms']\n",
    "    delta_z = pitch_pairs_df['next_z_150ms'] - pitch_pairs_df['z_150ms']\n",
    "    \n",
    "    pitch_pairs_df['tunnel_distance_150ms'] = np.sqrt(delta_x**2 + delta_z**2)\n",
    "\n",
    "    # --- 4. Aggregate Metrics ---\n",
    "\n",
    "    # Group by the pitch pair to calculate the mean distance and its consistency (Std Dev)\n",
    "    tunnel_results = pitch_pairs_df.groupby('pitch_pair').agg(\n",
    "        total_count=('pitch_pair', 'size'),\n",
    "        avg_distance_150ms=('tunnel_distance_150ms', 'mean'),\n",
    "        std_dev_distance=('tunnel_distance_150ms', 'std'),\n",
    "        \n",
    "        # Also include the mean location of the FIRST pitch in the pair (for visualization context)\n",
    "        avg_start_x=('x_150ms', 'mean'),\n",
    "        avg_start_z=('z_150ms', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Clean up results (e.g., limit decimal places for the final output)\n",
    "    for col in tunnel_results.columns:\n",
    "        if tunnel_results[col].dtype in ['float64']:\n",
    "            tunnel_results[col] = tunnel_results[col].round(4)\n",
    "\n",
    "    return tunnel_results\n",
    "\n",
    "# --- Execution Block ---\n",
    "\n",
    "INPUT_DIR = \"final_processed\"\n",
    "OUTPUT_DIR = 'web/public/assets'\n",
    "OUTPUT_FILE = \"tunnel_sequence_analysis.json\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting tunnel sequence analysis using files from: {INPUT_DIR}\")\n",
    "    \n",
    "    analysis_df = aggregate_tunnel_data(INPUT_DIR)\n",
    "    \n",
    "    if not analysis_df.empty:\n",
    "        # Convert the DataFrame to a JSON string\n",
    "        results_json = analysis_df.to_json(f'{OUTPUT_DIR}/{OUTPUT_FILE}', orient='records', indent=4, force_ascii=False)\n",
    "        \n",
    "        print(f\"\\n✅ Analysis Complete. Results saved to {OUTPUT_FILE}\")\n",
    "        print(f\"Total pitch pairs analyzed: {len(analysis_df)}\")\n",
    "        # Print a sample of the results\n",
    "        print(\"\\n--- Top 5 Pitch Pairs by Count ---\")\n",
    "        print(analysis_df.sort_values(by='total_count', ascending=False).head())\n",
    "    else:\n",
    "        print(\"\\n❌ Analysis failed or no data was processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591852ea-3659-44f8-ada4-33655ade314d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generates pitch trajectory data (batter_pov_trajectories.json)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "def generate_batter_pov_trajectories(input_dir: str, num_points: int = 40) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the average kinematic parameters for each pitch type and generates\n",
    "    a list of trajectory (x, z) points for visualization.\n",
    "    \n",
    "    Args:\n",
    "        input_dir: The directory containing the processed CSV files.\n",
    "        num_points: The number of points to generate for a smooth trajectory line.\n",
    "        \n",
    "    Returns:\n",
    "        A DataFrame containing the pitch type and its average trajectory points.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    input_files = glob.glob(os.path.join(input_dir, '*_final.csv'))\n",
    "    \n",
    "    if not input_files:\n",
    "        print(f\"No processed files found in {input_dir}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # --- 1. Load and Concatenate Data ---\n",
    "    for file in input_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file {file}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_data:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    master_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Ensure necessary columns are present (kinematic variables and t_plate)\n",
    "    required_cols = ['pitch_type', 'x0', 'z0', 'vx0', 'vz0', 'ax', 'az', 't_plate']\n",
    "    master_df = master_df.dropna(subset=required_cols)\n",
    "\n",
    "    # --- 2. Aggregate Average Kinematic Parameters (MODIFIED) ---\n",
    "    \n",
    "    # Create a new combined grouping column\n",
    "    master_df['pitcher_pitch_key'] = master_df['pitcher_id'].astype(str) + '_' + master_df['pitch_type']\n",
    "    \n",
    "    # Now group by the new combined key\n",
    "    avg_pitch_params = master_df.groupby('pitcher_pitch_key').agg(\n",
    "        avg_x_release=('x0', 'mean'),\n",
    "        avg_z_release=('z0', 'mean'),\n",
    "        avg_v_x=('vx0', 'mean'),\n",
    "        avg_v_z=('vz0', 'mean'),\n",
    "        avg_a_x=('ax', 'mean'),\n",
    "        avg_a_z=('az', 'mean'),\n",
    "        avg_t_plate=('t_plate', 'mean'),\n",
    "        pitcher_name=('pitcher_name', 'first'),\n",
    "        pitcher_team_code=('pitcher_team_code', 'first'),\n",
    "        pitcher_id=('pitcher_id', 'first'),\n",
    "        pitch_type=('pitch_type', 'first') \n",
    "    ).reset_index()\n",
    "\n",
    "    # --- 3. Generate Trajectory Points ---\n",
    "    \n",
    "    def generate_trajectory_points(row):\n",
    "        points = []\n",
    "        # Calculate time steps based on the average time to plate\n",
    "        t_plate = row['avg_t_plate']\n",
    "        t_step = t_plate / num_points\n",
    "        \n",
    "        # Define the tunnel end time point (150ms remaining)\n",
    "        T_150ms = t_plate - 0.150\n",
    "        \n",
    "        # Kinematic equations (using average values)\n",
    "        for i in range(num_points + 1):\n",
    "            t = i * t_step\n",
    "            \n",
    "            # Stop generating points if time exceeds t_plate\n",
    "            if t > t_plate:\n",
    "                break\n",
    "            \n",
    "            # P(t) = P_0 + V_0*t + 0.5*A*t^2\n",
    "            x_t = row['avg_x_release'] + row['avg_v_x'] * t + 0.5 * row['avg_a_x'] * (t**2)\n",
    "            z_t = row['avg_z_release'] + row['avg_v_z'] * t + 0.5 * row['avg_a_z'] * (t**2)\n",
    "            \n",
    "            # Flag the points that occur after the tunnel point (T_150ms)\n",
    "            is_tunnel_end = t >= T_150ms\n",
    "            \n",
    "            points.append({\n",
    "                't': round(t, 4), \n",
    "                'x': round(x_t, 4), \n",
    "                'z': round(z_t, 4), \n",
    "                'is_tunnel_end': is_tunnel_end\n",
    "            })\n",
    "        return points\n",
    "\n",
    "    # Apply function to get a list of points for each pitch type\n",
    "    avg_pitch_params['avg_trajectory'] = avg_pitch_params.apply(\n",
    "        generate_trajectory_points, axis=1\n",
    "    )\n",
    "\n",
    "    #return avg_pitch_params[['pitcher_id', 'pitcher_team_code', 'pitcher_name', 'pitch_type', 'avg_trajectory']]\n",
    "    return avg_pitch_params\n",
    "\n",
    "# --- Execution Block ---\n",
    "\n",
    "INPUT_DIR = \"final_processed\"\n",
    "OUTPUT_DIR = 'web/public/assets'\n",
    "OUTPUT_FILE = \"batter_pov_trajectories.json\"\n",
    "POINTS_PER_TRAJECTORY = 40 # Set the resolution of the visualization\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting Batter POV Trajectory generation using files from: {INPUT_DIR}\")\n",
    "    \n",
    "    trajectory_df = generate_batter_pov_trajectories(INPUT_DIR, POINTS_PER_TRAJECTORY)\n",
    "    \n",
    "    if not trajectory_df.empty:\n",
    "        # 1. EXPLODE: Convert the nested list column 'avg_trajectory' into multiple rows.\n",
    "        exploded_df = trajectory_df.explode('avg_trajectory')\n",
    "\n",
    "        # 2. NORMALIZE: Convert the dictionary in the exploded column into new, flat columns (t, x, z, is_tunnel_end).\n",
    "        # We also need to keep the identifiers (pitcher/pitch info) from the original row.\n",
    "        tidy_points = pd.json_normalize(exploded_df['avg_trajectory'])\n",
    "        \n",
    "        # 3. CONCATENATE: Join the new flat columns with the existing identifiers.\n",
    "        # Drop the original nested column before joining.\n",
    "        identifiers_df = exploded_df.drop(columns=['avg_trajectory']).reset_index(drop=True)\n",
    "        tidy_points = tidy_points.reset_index(drop=True)\n",
    "        \n",
    "        # Concatenate the identifiers (pitcher_id, pitch_type, etc.) with the point data (x, z, t)\n",
    "        tidy_df = pd.concat([identifiers_df, tidy_points], axis=1)\n",
    "        \n",
    "        # 4. SELECT FINAL COLUMNS: Keep only the columns necessary for Vega visualization and indexing\n",
    "        final_vega_df = tidy_df[[\n",
    "            'pitcher_id', 'pitcher_name', 'pitch_type', 'pitcher_team_code', \n",
    "            't', 'x', 'z', 'is_tunnel_end'\n",
    "        ]]\n",
    "        \n",
    "        # 5. Save the Tidy DataFrame to JSON\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        final_vega_df.to_json(f'{OUTPUT_DIR}/{OUTPUT_FILE}', orient='records', indent=4, force_ascii=False)\n",
    "\n",
    "        print(f\"\\n✅ Trajectory Generation Complete. Tidy Vega-Lite data saved to{OUTPUT_FILE}\")\n",
    "        print(f\"Total data points exported: {len(final_vega_df)}\")\n",
    "    else:\n",
    "        print(\"\\n❌ Trajectory generation failed or no data was processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b9897-5dd6-46b8-93f1-a2159ac49289",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# DRAFT audit final_processed columns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "REQUIRED_COLS = [\n",
    "    \"game_id\",\n",
    "    \"season_year\",\n",
    "    \"season_type\",\n",
    "    \"pitch_id\",\n",
    "    \"inning\",\n",
    "    \"is_batter_home\",\n",
    "    \"out\",\n",
    "    \"ball\",\n",
    "    \"strike\",\n",
    "    \"is_final_pitch\",\n",
    "    \"final_pitch_result\", # pitch_result\n",
    "    \"pa_result_short\",\n",
    "    \"pa_result_runs\",\n",
    "    \"pa_result_base1\",\n",
    "    \"base1\"\n",
    "]\n",
    "\n",
    "missing = set(REQUIRED_COLS) - set(df.columns)\n",
    "assert not missing, f\"Missing columns: {missing}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342a93b-b74b-4a47-a5c8-12c731089770",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# audit row/pitch count per season\n",
    "\n",
    "# ==============================\n",
    "# SEASON          | PITCH COUNT \n",
    "# ------------------------------\n",
    "# 2023 POST       |        4,588\n",
    "# 2023 PRE        |       19,746\n",
    "# 2023 REG        |      219,990\n",
    "# 2024 POST       |        5,162\n",
    "# 2024 PRE        |       13,338\n",
    "# 2024 REG        |      223,626\n",
    "# 2025 POST       |        4,678\n",
    "# 2025 PRE        |       12,416\n",
    "# 2025 REG        |      217,857\n",
    "# ------------------------------\n",
    "# TOTAL           |      721,401\n",
    "# ==============================\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# The base directory where your processed files are stored\n",
    "BASE_DIR = \"final_processed\"\n",
    "\n",
    "def count_pitches_per_season(base_dir):\n",
    "    # Use a dictionary to store totals: { '2025_reg': 1500, '2025_post': 200 ... }\n",
    "    stats = defaultdict(int)\n",
    "    \n",
    "    print(f\"Counting pitches in {base_dir}...\")\n",
    "\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        # Ignore hidden folders\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "        \n",
    "        csv_files = [f for f in files if f.endswith('.csv')]\n",
    "        if not csv_files:\n",
    "            continue\n",
    "\n",
    "        # Get relative path to identify Year and Season Type\n",
    "        rel_path = os.path.relpath(root, base_dir)\n",
    "        path_parts = rel_path.split(os.sep)\n",
    "        \n",
    "        # Determine Year and Type\n",
    "        year = path_parts[0]\n",
    "        s_type = path_parts[1] if len(path_parts) > 1 else 'reg'\n",
    "        \n",
    "        season_key = f\"{year} {s_type.upper()}\"\n",
    "\n",
    "        for filename in csv_files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                # We only load the index/header to count rows quickly\n",
    "                # This is much faster than df = pd.read_csv(file_path)\n",
    "                count = len(pd.read_csv(file_path, usecols=[0])) \n",
    "                stats[season_key] += count\n",
    "            except Exception as e:\n",
    "                print(f\"Error counting {file_path}: {e}\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "# --- Run and Print Results ---\n",
    "pitch_stats = count_pitches_per_season(BASE_DIR)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"{'SEASON':<15} | {'PITCH COUNT':<12}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "total_all_time = 0\n",
    "for season in sorted(pitch_stats.keys()):\n",
    "    count = pitch_stats[season]\n",
    "    total_all_time += count\n",
    "    print(f\"{season:<15} | {count:>12,}\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"{'TOTAL':<15} | {total_all_time:>12,}\")\n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75d41b7-9764-4bf2-b875-a0ece79a58c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# audit game count\n",
    "\n",
    "#             Games Found  Target        Status  Avg Pitchers/Game\n",
    "# season_year                                                      \n",
    "# 2023.0               721     720  MISSING DATA               9.50\n",
    "# 2024.0               723     720  MISSING DATA               9.78\n",
    "# 2025.0               720     720            OK               9.76\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Grab every .csv file in the final_processed directory tree\n",
    "all_paths = glob.glob(\"final_processed/**/*.csv\", recursive=True)\n",
    "\n",
    "# 2. Filter out any path that contains '.ipynb_checkpoints'\n",
    "# We also check that it's a file, just to be safe\n",
    "clean_paths = [\n",
    "    p for p in all_paths \n",
    "    if \".ipynb_checkpoints\" not in p and os.path.isfile(p)\n",
    "]\n",
    "\n",
    "print(f\"Found {len(clean_paths)} valid game files. Ignoring hidden checkpoint folders.\")\n",
    "\n",
    "# 3. Load and combine into one master DataFrame\n",
    "# This creates a list of DataFrames and then stacks them\n",
    "df_list = [pd.read_csv(f) for f in clean_paths]\n",
    "master_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "print(f\"Master DataFrame loaded: {len(master_df):,} total pitches.\")\n",
    "\n",
    "# --- 1. Game Count Validation ---\n",
    "# Filter for regular season and count unique game IDs per year\n",
    "reg_df = master_df[master_df['season_type'] == 'reg']\n",
    "game_counts = reg_df.groupby('season_year')['game_id'].nunique()\n",
    "\n",
    "# --- 2. Pitcher Statistics ---\n",
    "# First, count unique pitchers per individual game\n",
    "pitchers_per_game = reg_df.groupby(['season_year', 'game_id'])['pitcher_id'].nunique()\n",
    "\n",
    "# Then, find the mean of those counts per year\n",
    "mean_pitchers = pitchers_per_game.groupby('season_year').mean()\n",
    "\n",
    "# --- 3. Display Results ---\n",
    "validation_report = pd.DataFrame({\n",
    "    'Games Found': game_counts,\n",
    "    'Target': 720,\n",
    "    'Status': ['OK' if count == 720 else 'MISSING DATA' for count in game_counts],\n",
    "    'Avg Pitchers/Game': mean_pitchers.round(2)\n",
    "})\n",
    "\n",
    "print(\"KBO Season Validation Report:\")\n",
    "print(validation_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602c4b4-0cac-4a27-9d45-c3b2afe801c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# audit innings count\n",
    "\n",
    "# ❌ Found 5 incomplete files:\n",
    "# --------------------------------------------------------------------------------\n",
    "# Filename                                 | Inn   | Pitches  | Reason\n",
    "# --------------------------------------------------------------------------------\n",
    "# 20230404HTKT02023_final.csv              | 4     | 151      | Innings: 4\n",
    "# 20240423SKLT02024_final.csv              | 4     | 140      | Innings: 4\n",
    "# 20240629SSKT02024_final.csv              | 4     | 165      | Innings: 4\n",
    "# 20240820LTHT02024_final.csv              | 4     | 129      | Innings: 4\n",
    "# 20240312OBLT02024_final.csv              | 3     | 86       | Innings: 3\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configurations ---\n",
    "INPUT_DIR = \"final_processed\"\n",
    "MIN_INNINGS = 5\n",
    "MIN_PITCHES = 20\n",
    "\n",
    "def audit_games(input_dir):\n",
    "    incomplete_games = []\n",
    "    \n",
    "    print(f\"Auditing files in {input_dir}...\")\n",
    "    \n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        # Ignore hidden folders\n",
    "        dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "        \n",
    "        for filename in files:\n",
    "            if not filename.endswith('.csv'):\n",
    "                continue\n",
    "                \n",
    "            file_path = os.path.join(root, filename)\n",
    "            \n",
    "            try:\n",
    "                # We only load 'inning' to save memory/speed\n",
    "                df = pd.read_csv(file_path, usecols=['inning'])\n",
    "                \n",
    "                if df.empty:\n",
    "                    incomplete_games.append((filename, 0, 0, \"Empty File\"))\n",
    "                    continue\n",
    "\n",
    "                max_inning = df['inning'].max()\n",
    "                total_pitches = len(df)\n",
    "                \n",
    "                # Logic check\n",
    "                if max_inning < MIN_INNINGS or total_pitches < MIN_PITCHES:\n",
    "                    reason = []\n",
    "                    if max_inning < MIN_INNINGS: reason.append(f\"Innings: {max_inning}\")\n",
    "                    if total_pitches < MIN_PITCHES: reason.append(f\"Pitches: {total_pitches}\")\n",
    "                    \n",
    "                    incomplete_games.append((filename, max_inning, total_pitches, \" & \".join(reason)))\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Could not check {filename}: {e}\")\n",
    "\n",
    "    return incomplete_games\n",
    "\n",
    "# --- Execute Audit ---\n",
    "bad_files = audit_games(INPUT_DIR)\n",
    "\n",
    "if not bad_files:\n",
    "    print(\"✅ All files passed the audit!\")\n",
    "else:\n",
    "    print(f\"\\n❌ Found {len(bad_files)} incomplete files:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Filename':<40} | {'Inn':<5} | {'Pitches':<8} | {'Reason'}\")\n",
    "    print(\"-\" * 80)\n",
    "    for name, inn, count, reason in bad_files:\n",
    "        print(f\"{name[:40]:<40} | {inn:<5} | {count:<8} | {reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc0b0b-6808-4fdf-b2a0-ce81c0b2fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more pre-processing for RE24 matrix (kbo_master_analytics.parquet)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pyarrow\n",
    "\n",
    "# 1. Load the data (Using your clean_paths logic)\n",
    "all_paths = glob.glob(\"final_processed/**/*.csv\", recursive=True)\n",
    "clean_paths = [p for p in all_paths if \".ipynb_checkpoints\" not in p and os.path.isfile(p)]\n",
    "\n",
    "print(f\"Processing {len(clean_paths)} games...\")\n",
    "\n",
    "df_list = []\n",
    "for f in clean_paths:\n",
    "    temp_df = pd.read_csv(f, dtype={'base_state': str, 'pitch_id': str})\n",
    "    df_list.append(temp_df)\n",
    "\n",
    "master_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 1. Count runs scored only on final pitch (most likely to have scored the runs)\n",
    "master_df['runs_scored_on_play'] = np.where(\n",
    "    master_df['is_final_pitch'] == True, \n",
    "    master_df['pa_result_runs'].fillna(0), \n",
    "    0\n",
    ")\n",
    "\n",
    "# 2. Generate IDs and States\n",
    "master_df['half_inning_id'] = (\n",
    "    master_df['game_id'] + \"_\" + \n",
    "    master_df['inning'].astype(str) + \"_\" + \n",
    "    master_df['is_batter_home'].astype(str)\n",
    ")\n",
    "\n",
    "# 3. Calculate Runs to End of Inning (Corrected for double-counting)\n",
    "# Only count runs on the pitch where the PA actually ended\n",
    "master_df['runs_on_this_pitch'] = np.where(master_df['is_final_pitch'], master_df['pa_result_runs'], 0)\n",
    "\n",
    "# Total runs per half-inning\n",
    "inning_totals = master_df.groupby('half_inning_id')['runs_on_this_pitch'].sum().reset_index()\n",
    "inning_totals.columns = ['half_inning_id', 'total_inning_runs']\n",
    "\n",
    "# Merge and calculate \"Future Runs\"\n",
    "master_df = master_df.merge(inning_totals, on='half_inning_id', how='left')\n",
    "master_df['runs_so_far'] = master_df.groupby('half_inning_id')['runs_on_this_pitch'].cumsum() - master_df['runs_on_this_pitch']\n",
    "master_df['runs_to_end_of_inning'] = master_df['total_inning_runs'] - master_df['runs_so_far']\n",
    "\n",
    "# List of columns that should definitely be strings to avoid the Arrow error\n",
    "string_cols = ['pitch_id', 'game_id', 'half_inning_id', 'base_state', 're_state']\n",
    "\n",
    "for col in string_cols:\n",
    "    if col in master_df.columns:\n",
    "        # Convert to string and fill any missing values with an empty string\n",
    "        master_df[col] = master_df[col].astype(str).replace('nan', '')\n",
    "\n",
    "# 4. Save for the future!\n",
    "# If you don't have 'pyarrow' installed, use .to_csv(\"kbo_master_analytics.csv\", index=False)\n",
    "try:\n",
    "    master_df.to_parquet(\"kbo_master_analytics.parquet\", index=False)\n",
    "    print(\"✅ Success! Saved to 'kbo_master_analytics.parquet'\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Parquet failed:\", repr(e))\n",
    "    master_df.to_csv(\"kbo_master_analytics.csv\", index=False, encoding='utf-8-sig')\n",
    "    print(\"✅ Success! Saved to 'kbo_master_analytics.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9bf39c-714c-4164-b502-bf4e17c08527",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# RE24 by base–outs matrix\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load the analytics-ready file\n",
    "df = pd.read_parquet(\"kbo_master_analytics.parquet\")\n",
    "\n",
    "# 2. Filter for Regular Season only (to ensure the values reflect standard play)\n",
    "reg_df = df[df['season_type'] == 'reg'].copy()\n",
    "\n",
    "# 3. Create the RE24 Matrix\n",
    "# Rows = Outs, Columns = Base State (000, 100, etc.)\n",
    "re24_matrix = reg_df.groupby(['out', 'base_state'])['runs_to_end_of_inning'].mean().unstack()\n",
    "\n",
    "# 4. Clean up for display\n",
    "re24_matrix.index.name = 'Outs'\n",
    "re24_matrix.columns.name = 'Base State'\n",
    "\n",
    "print(\"KBO RE24 Matrix (Average Runs Expected per State):\")\n",
    "print(re24_matrix.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c126dc45-4079-47b7-be0d-d6576613fe16",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RE by (balls–strikes–outs–base_state)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import common as cmn\n",
    "\n",
    "# Transition mapping for bases when a Walk occurs\n",
    "base_map = {\n",
    "    '000': '100', '100': '110', '010': '110', '001': '101',\n",
    "    '110': '111', '101': '111', '011': '111', '111': '111'\n",
    "}\n",
    "\n",
    "# KBO ABS Constants (2025 Update)\n",
    "# Plate width 17\" + 2cm buffer on each side + ball radius grazing\n",
    "ABS_WIDTH_FT = 1.8  # Approx (-0.9 to 0.9)\n",
    "\n",
    "def get_state_re(out, base_state, count):\n",
    "    \"\"\"\n",
    "    Helper to safely get the RE value. \n",
    "    If 3 outs, the inning is over, so value is always 0.\n",
    "    \"\"\"\n",
    "    if out >= 3:\n",
    "        return 0.0\n",
    "    return re_lookup_dict.get((out, base_state, count), 0.0)\n",
    "    \n",
    "# Map the RE values to these states\n",
    "def get_state_val(o, base):\n",
    "    if o >= 3: return 0.0\n",
    "    return re24_dict.get(int(o), {}).get(base, 0.0)\n",
    "    \n",
    "def calculate_transitions(row):\n",
    "    b, s, o = int(row['ball']), int(row['strike']), int(row['out'])\n",
    "    base = row['base_state']\n",
    "    \n",
    "    # --- IF STRIKE ---\n",
    "    if s == 2: # Strikeout\n",
    "        s_next = (o + 1, base, \"0-0\", 0) # Next out, same base, reset count\n",
    "    else:\n",
    "        s_next = (o, base, f\"{b}-{s+1}\", 0)\n",
    "        \n",
    "    # --- IF BALL ---\n",
    "    if b == 3: # Walk\n",
    "        next_base = base_map.get(base, '100')\n",
    "        run = 1 if base == '111' else 0 # Bases loaded walk scores a run\n",
    "        s_ball = (o, next_base, \"0-0\", run)\n",
    "    else:\n",
    "        s_ball = (o, base, f\"{b+1}-{s}\", 0)\n",
    "        \n",
    "    return s_next, s_ball\n",
    "\n",
    "def get_val(state_tuple):\n",
    "    out, base, cnt, run_scored = state_tuple\n",
    "    # If it reached 3 outs, the value of the next state is 0\n",
    "    if out >= 3:\n",
    "        return 0.0 + run_scored\n",
    "    return re_lookup_dict.get((out, base, cnt), 0.0) + run_scored\n",
    "\n",
    "def get_take_value(row):\n",
    "    # Rule: If any part of the ball touches the zone, it's a strike\n",
    "    # Vertical zone is adaptive (using 180cm avg if height is missing)\n",
    "    height = row.get('batter_height_cm', 180)\n",
    "    top = height * 0.5575 / 30.48\n",
    "    bottom = height * 0.2704 / 30.48\n",
    "    \n",
    "    is_strike = (abs(row['plate_x_ft']) <= ABS_WIDTH_FT/2) and \\\n",
    "                (bottom <= row['plate_z_ft'] <= top)\n",
    "    \n",
    "    return row['v_strike'] if is_strike else row['v_ball']\n",
    "\n",
    "master_df = pd.read_parquet(\"kbo_master_analytics.parquet\")\n",
    "\n",
    "#Create a lookup for your RE24 Matrix (The one with 24 states: 3 outs x 8 base states)\n",
    "re24_dict = re24_matrix.to_dict() # Assuming it's in a dict format or DataFrame\n",
    "\n",
    "# Create out_after column\n",
    "# 1. Sort to ensure sequence is perfect\n",
    "#master_df = master_df.sort_values(['game_id', 'inning', 'batter_lineup_pos', 'ball_count'])\n",
    "\n",
    "# 2. Shift the states to see what happened \"After\"\n",
    "master_df['out_after'] = master_df.groupby(['game_id', 'inning'])['out'].shift(-1)\n",
    "master_df['base_state_after'] = master_df.groupby(['game_id', 'inning'])['base_state'].shift(-1)\n",
    "\n",
    "# 3. Handle the 'End of Inning'\n",
    "master_df['out_after'] = master_df['out_after'].fillna(3)\n",
    "master_df['base_state_after'] = master_df['base_state_after'].fillna('000')\n",
    "\n",
    "# 4. Map the RE values to these states\n",
    "master_df['re_before'] = master_df.apply(lambda x: get_state_val(x['out'], x['base_state']), axis=1)\n",
    "master_df['re_after'] = master_df.apply(lambda x: get_state_val(x['out_after'], x['base_state_after']), axis=1)\n",
    "\n",
    "# 5. Calculate Actual Run Value (RV)\n",
    "master_df['rv_actual'] = (master_df['re_after'] - master_df['re_before']) + master_df['runs_scored_on_play']\n",
    "\n",
    "\n",
    "# 1. Ensure the 'count' string exists\n",
    "# We use .astype(int) to make sure 3.0 becomes \"3\"\n",
    "master_df['count'] = (\n",
    "    master_df['ball'].astype(int).astype(str) + \"-\" + \n",
    "    master_df['strike'].astype(int).astype(str)\n",
    ")\n",
    "\n",
    "# 2. Group by the 'Triple Crown' of context: Outs, Base State, and Count\n",
    "# We calculate the mean of 'runs_to_end_of_inning' for each bucket\n",
    "re_count_table = (\n",
    "    master_df.groupby(['out', 'base_state', 'count', 'ball', 'strike'])['runs_to_end_of_inning']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 3. Rename the column for clarity\n",
    "re_count_table.rename(columns={'runs_to_end_of_inning': 're_val'}, inplace=True)\n",
    "\n",
    "# # 4. Quick Check: Let's see the value of a 0-0 count vs a 3-0 count with 0 outs, bases empty\n",
    "# check = re_count_table[\n",
    "#     (re_count_table['out'] == 0) & \n",
    "#     (re_count_table['base_state'] == '000') & \n",
    "#     (re_count_table['count'].isin(['0-0', '3-0', '0-2']))\n",
    "# ]\n",
    "# print(\"Baseline RE for specific counts (0 Out, Bases Empty):\")\n",
    "# print(check)\n",
    "\n",
    "# Create a dictionary for ultra-fast lookup\n",
    "# Key: (out, base_state, count) -> Value: re_val\n",
    "re_lookup_dict = re_count_table.set_index(['out', 'base_state', 'count'])['re_val'].to_dict()\n",
    "\n",
    "# Apply transitions to our 288-cell table\n",
    "transitions = re_count_table.apply(calculate_transitions, axis=1)\n",
    "re_count_table['next_state_strike'], re_count_table['next_state_ball'] = zip(*transitions)\n",
    "\n",
    "re_count_table['re_if_strike'] = re_count_table['next_state_strike'].apply(get_val)\n",
    "re_count_table['re_if_ball'] = re_count_table['next_state_ball'].apply(get_val)\n",
    "\n",
    "# THE PAYOFF: The value of the change\n",
    "re_count_table['v_strike'] = re_count_table['re_if_strike'] - re_count_table['re_val']\n",
    "re_count_table['v_ball'] = re_count_table['re_if_ball'] - re_count_table['re_val']\n",
    "\n",
    "# Keep only the columns we need for the master join\n",
    "pitch_value_lookup = re_count_table[['out', 'base_state', 'count', 'v_strike', 'v_ball']]\n",
    "\n",
    "master_df = master_df.merge(pitch_value_lookup, on=['out', 'base_state', 'count'], how='left')\n",
    "\n",
    "master_df['v_take'] = master_df.apply(get_take_value, axis=1)\n",
    "\n",
    "\n",
    "# 1. Filter for all Swings in your 3-year data\n",
    "swings_df = master_df[master_df['pitch_result'].isin(cmn.SWING_CODES)].copy()\n",
    "\n",
    "# 2. Target: 1 if they missed (Whiff), 0 if they made contact (Foul or BIP)\n",
    "swings_df['is_whiff'] = swings_df['pitch_result'].isin(cmn.WHIFF_CODES).astype(int)\n",
    "\n",
    "# 3. Features: What makes a batter miss?\n",
    "features = ['plate_x_ft', 'plate_z_ft', 'pitch_speed_kph', 'ax', 'az', 'ball', 'strike']\n",
    "swings_df = swings_df.dropna(subset=features) # Clean any NaNs in features\n",
    "\n",
    "# 4. Train the Model\n",
    "whiff_model = XGBClassifier(n_estimators=100, max_depth=4, learning_rate=0.1)\n",
    "whiff_model.fit(swings_df[features], swings_df['is_whiff'])\n",
    "\n",
    "# 5. Apply to all pitches to get the \"Probability of Whiff\"\n",
    "master_df['prob_whiff'] = whiff_model.predict_proba(master_df[features].fillna(0))[:, 1]\n",
    "\n",
    "# Save current progress to a new file\n",
    "master_df.to_parquet(\"kbo_master_with_probs.parquet\", index=False)\n",
    "#master_df.to_csv(\"kbo_master_with_probs.csv\", index=False, encoding='utf-8-sig')\n",
    "print(\"✅ Progress saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db636a-1bea-447d-a8c0-625fbbd13ff7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import common as cmn\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "\n",
    "features = ['plate_x_ft', 'plate_z_ft', 'pitch_speed_kph', 'ax', 'az', 'ball', 'strike']\n",
    "\n",
    "def get_rv(row):\n",
    "    # Current State Value\n",
    "    val_before = re24_dict.get(row['out'], {}).get(row['base_state'], 0)\n",
    "    \n",
    "    # After State Value\n",
    "    # If 3 outs occurred, the state value is 0\n",
    "    if row['out_after'] >= 3:\n",
    "        val_after = 0\n",
    "    else:\n",
    "        val_after = re24_dict.get(row['out_after'], {}).get(row['base_state_after'], 0)\n",
    "        \n",
    "    # The actual Run Value of that specific event\n",
    "    return (val_after - val_before) + row['runs_scored_on_play']\n",
    "\n",
    "def calculate_rv_actual(row):\n",
    "    val_before = re24_dict.get(row['out'], {}).get(row['base_state'], 0)\n",
    "    if row['out_after'] >= 3:\n",
    "        val_after = 0\n",
    "    else:\n",
    "        val_after = re24_dict.get(row['out_after'], {}).get(row['base_state_after'], 0)\n",
    "    return (val_after - val_before) + row['runs_scored_on_play']\n",
    "\n",
    "# 1. Load the checkpoint\n",
    "df = pd.read_parquet(\"kbo_master_with_probs.parquet\")\n",
    "\n",
    "# 2. Train the BIP (Ball In Play) Model\n",
    "# We only care about 'H' (Hits/In-Play) for this specific model\n",
    "bip_df = df[df['pitch_result'] == 'H'].copy()\n",
    "\n",
    "bip_model = XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.1)\n",
    "bip_model.fit(bip_df[features], bip_df['rv_actual'])\n",
    "\n",
    "# 3. \"Expected Run Value\" (xRV) of contact at that location\n",
    "df['ev_bip'] = bip_model.predict(df[features].fillna(0))\n",
    "\n",
    "\n",
    "# 4. FINAL CALCULATION: The Decision Score\n",
    "# Swing Value: Weighting the whiff vs. the expected contact value\n",
    "df['v_swing'] = (df['prob_whiff'] * df['v_strike']) + ((1 - df['prob_whiff']) * df['ev_bip'])\n",
    "\n",
    "# Decision Score: \n",
    "# If they swung: How much better was swinging than taking?\n",
    "# If they took: How much better was taking than swinging?\n",
    "df['decision_score'] = np.where(\n",
    "    df['pitch_result'].isin(cmn.SWING_CODES),\n",
    "    df['v_swing'] - df['v_take'], # Swung: (Value of Swing) - (Alternative: Value of Take)\n",
    "    df['v_take'] - df['v_swing']  # Took: (Value of Take) - (Alternative: Value of Swing)\n",
    ")\n",
    "\n",
    "# 5. Save the Master Data with Scores\n",
    "df.to_parquet(\"kbo_decision_final.parquet\", index=False)\n",
    "print(\"✅ Final Decision Scores calculated and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a92e6-25df-4410-96ce-6e4e91f502cd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the Leaderboard\n",
    "leaderboard = df.groupby(['batter_team_code', 'batter_id', 'batter_name']).agg({\n",
    "    'decision_score': 'mean',\n",
    "    'ball_count': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "# Filter for qualified hitters\n",
    "qualified = leaderboard[leaderboard['ball_count'] > 1000].sort_values('decision_score', ascending=False)\n",
    "\n",
    "print(\"KBO Plate Discipline Leaders (Top 10):\")\n",
    "print(qualified.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182df593-459a-42f6-a313-9e4c13ad0d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2023~2025 KBO Plate Discipline\n",
    "\n",
    "# Runs per 100 decisions\n",
    "df[\"decision_score_100\"] = df[\"decision_score\"] * 100\n",
    "\n",
    "# Create the Leaderboard\n",
    "leaderboard = df.groupby(['batter_team_code', 'batter_name']).agg({\n",
    "    'decision_score_100': 'mean',\n",
    "    'ball_count': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "# Filter for qualified hitters\n",
    "qualified = leaderboard[leaderboard['ball_count'] > 1000].sort_values('decision_score_100', ascending=False)\n",
    "\n",
    "print(\"KBO Plate Discipline Leaders (Top 10):\")\n",
    "print(qualified.head(10))\n",
    "\n",
    "\n",
    "# 1. Group by Team and Batter\n",
    "# We use 'mean' for the score and 'count' for the sample size\n",
    "team_leaderboard = df.groupby(['batter_team_code', 'batter_name']).agg({\n",
    "    'decision_score_100': 'mean',\n",
    "    'ball_count': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "# 2. Filter for Qualification\n",
    "qualified_team = team_leaderboard[team_leaderboard['ball_count'] > 1000].copy()\n",
    "\n",
    "# 3. Sort and Get Top 10 per Team\n",
    "# We sort by team (alphabetical) and then by score (highest first)\n",
    "qualified_team = qualified_team.sort_values(['batter_team_code', 'decision_score_100'], ascending=[True, False])\n",
    "\n",
    "# 4. Use groupby + head(10) to slice the best 10 for every team\n",
    "top_10_per_team = qualified_team.groupby('batter_team_code').head(10)\n",
    "\n",
    "# Save the Top 10 per Team to a CSV\n",
    "top_10_per_team.to_csv(\"kbo_team_leaderboards.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "# If you also want to save the full player ranking list\n",
    "qualified_team.to_csv(\"kbo_full_rankings.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"📊 Leaderboards saved to CSV successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fea68c-2de5-4b50-a787-9a931edac7e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2025 KBO Plate Discipline\n",
    "\n",
    "year = 2025\n",
    "\n",
    "batter_year_df = df[master_df[\"season_year\"] == year].copy()\n",
    "\n",
    "batter_year_df = (\n",
    "    batter_year_df\n",
    "        .groupby(['batter_team_code', 'batter_name'])\n",
    "        .agg({\n",
    "            'decision_score': 'mean',\n",
    "            'ball_count': 'count'\n",
    "        }).reset_index())\n",
    "\n",
    "batter_year_df[\"decision_score_100\"] = batter_year_df[\"decision_score\"] * 100\n",
    "batter_year_df.drop('decision_score', axis=1, inplace=True)\n",
    "\n",
    "batter_year_df = batter_year_df[batter_year_df[\"ball_count\"] >= 200]\n",
    "\n",
    "batter_year_df.sort_values(\"decision_score_100\", ascending=False).head(10)\n",
    "batter_year_df = batter_year_df.sort_values(['batter_team_code', 'decision_score_100'], ascending=[True, False])\n",
    "top_10_per_team = batter_year_df.groupby('batter_team_code').head(10)\n",
    "\n",
    "# Save the Top 10 per Team to a CSV\n",
    "top_10_per_team.to_csv(\"kbo_team_leaderboards_2025.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "# If you also want to save the full player ranking list\n",
    "batter_year_df.to_csv(\"kbo_full_rankings_2025.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"📊 Leaderboards saved to CSV successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca37f38-adc6-4689-af1a-9f2ee0e014c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
